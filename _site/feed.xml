<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xichen Pan</title>
    <description>hhh</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 13 Apr 2020 03:53:31 -0600</pubDate>
    <lastBuildDate>Mon, 13 Apr 2020 03:53:31 -0600</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Intro to Graph</title>
        <description>&lt;h3 id=&quot;basic-terminology&quot;&gt;Basic terminology&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;V Nodes/ Vertices:&lt;/em&gt;&lt;/strong&gt; A set of n elements with unique identifiers (1, 2, 3, … , n).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;E Edges:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For undirected graph: each edge is a set of exactly two nodes, e.g. e = {u, v}. u and v are nodes and we say e connects u and v. We also say u and v are adjacent or neighbors.&lt;/p&gt;

&lt;p&gt;For directed graph: each edge is an ordered pair, e.g. e = &amp;lt;u, v&amp;gt;. We say u and v are adjacent. U is an in-neighbor of v and v is an out-neighbor of u. (u –&amp;gt; v).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Adjacent&lt;/em&gt;&lt;/strong&gt; can be the property between vertex and vertex or between edge and edge. e.g., 1 and 3 are adjacent. (1,2) and (2,5) are adjacent.&lt;/p&gt;

&lt;p&gt;Incident can be the property between vertex and edge. e.g., 1 and (1,3) are incident.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Loop/ self-loop:&lt;/em&gt;&lt;/strong&gt; the edge is the form e = uu, which means the edge connect one node and the node itself.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Degree of vertex:&lt;/em&gt;&lt;/strong&gt;  The number of edges that touch v. It is also known as the number neighbors of v.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Path:&lt;/em&gt;&lt;/strong&gt; A sequence of nodes v_0, v_1, …, v_k such there exists k edges e_1, e_2, …, e_k where e_i connect v_i-1 to v_i.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Simple path:&lt;/em&gt;&lt;/strong&gt; A path where all nodes are unique. Then since there are k edges, k is the length of the path.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Cycle:&lt;/em&gt;&lt;/strong&gt; A path where the start and the end vertex are the same, v_0 = v_k.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A simple cycle:&lt;/em&gt;&lt;/strong&gt; A cycle where all vertex except v_0 and v_k are unique. k is the length of the cycle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Size of the graph:&lt;/em&gt;&lt;/strong&gt; |G| = number of vertex |V| = n&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sub-graph:&lt;/em&gt;&lt;/strong&gt; G’ = (V’, E’) is a sub-graph of G = (V, E) if &lt;script type=&quot;math/tex&quot;&gt;V' \subset V and E' \subset E&lt;/script&gt;. Removing an edge from G results in the subgraph (V, E \ {e})&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Induced subgraph:&lt;/em&gt;&lt;/strong&gt; Let G = (V, E). And let &lt;script type=&quot;math/tex&quot;&gt;S \subset V&lt;/script&gt; be any subset of vertices of G. Then the induced subgraph G[S] is the graph whose vertices set is S and the edge set contains all edges which connect nodes in S in G. Removing a node from G results in induced graph.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Connectivity in an undirected graph:&lt;/em&gt;&lt;/strong&gt; u is connected to v (u ~ v) if there exists a path from u to v. Note: otherwise, we can only say edge e connects the nodes u and v.&lt;/p&gt;

&lt;p&gt;G is a connected graph if for every &lt;script type=&quot;math/tex&quot;&gt;u,v \in V, u~v&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;to-be-finished&quot;&gt;To be finished:&lt;/h5&gt;

&lt;p&gt;Connected component, Forests and trees.&lt;/p&gt;

&lt;h3 id=&quot;basic-properties&quot;&gt;Basic properties&lt;/h3&gt;

&lt;p&gt;A graph with n nodes and m edges. &lt;strong&gt;&lt;em&gt;The range of m:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For directed graph: The max number of edges is (n)&lt;em&gt;(n-1). The max situation is when every node connects to all the other nodes. There are n such situations so the total number is n&lt;/em&gt;(n-1).&lt;/p&gt;

&lt;p&gt;For undirected graph, e = (u,v) and x = (v,u) are the same edge and be counted as one edge. So the total number is n*(n-1)/2.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The number of degree:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For an undirected graph: &lt;script type=&quot;math/tex&quot;&gt;\sum_{v\in V} deg(v) = 2m&lt;/script&gt;. It means that in an undirected graph, there are 2m degrees for all vertices. Because for each edge, it connects two vertices so it provides 2 degrees in total.&lt;/p&gt;

&lt;p&gt;For an directed graph: &lt;script type=&quot;math/tex&quot;&gt;\sum_{v\in V} in_deg(v) = m =  \sum_{v\in V} out_deg(v)&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;representation-of-graph&quot;&gt;Representation of Graph&lt;/h3&gt;

&lt;p&gt;Two ways: Adjacency matrix and Adjacent list.&lt;/p&gt;

&lt;p&gt;Adjacency matrix is an n*n matrix where i, j entry contains 1 if edge connects i and j or 0 if no edge.&lt;/p&gt;

&lt;p&gt;Adjacency list: It lists all nodes in one column and following the nodes which are their neighbors.&lt;/p&gt;

&lt;p&gt;The complexity of two ways:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Adjacency List&lt;/th&gt;
      &lt;th&gt;Adjacency Matrix&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Space&lt;/td&gt;
      &lt;td&gt;O(m) Depend on the edge&lt;/td&gt;
      &lt;td&gt;O(n^2)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Accessing a node v/ Traversing all node&lt;/td&gt;
      &lt;td&gt;O(1) / O(n)&lt;/td&gt;
      &lt;td&gt;O(1) / O(n)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Accessing a edge e = (u,v)&lt;/td&gt;
      &lt;td&gt;O(|&lt;script type=&quot;math/tex&quot;&gt;\Gamma(u)&lt;/script&gt;|) # of neighbors of u&lt;/td&gt;
      &lt;td&gt;O(1), M[u, v]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Finding some neighbor of v&lt;/td&gt;
      &lt;td&gt;O(1)&lt;/td&gt;
      &lt;td&gt;O(n)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Traversing all edges/ vertices adjacent to a node u&lt;/td&gt;
      &lt;td&gt;O(&lt;script type=&quot;math/tex&quot;&gt;\Gamma(u)&lt;/script&gt;|)&lt;/td&gt;
      &lt;td&gt;O(n)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Traversing all edges&lt;/td&gt;
      &lt;td&gt;O(m)&lt;/td&gt;
      &lt;td&gt;O(n^2)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;application-of-dfs&quot;&gt;Application of dfs&lt;/h3&gt;

&lt;h5 id=&quot;finding-scc&quot;&gt;finding SCC&lt;/h5&gt;

&lt;p&gt;Strongly connected components (SCCs) is a max set of nodes, any two of the nodes are reachable from each other.&lt;/p&gt;

&lt;p&gt;Alg:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Run dfs on G&lt;/li&gt;
  &lt;li&gt;Flip G’s edges to create G^T&lt;/li&gt;
  &lt;li&gt;Run dfs on G^T by traversing nodes in a decreasing order of finish time generated in step 1&lt;/li&gt;
  &lt;li&gt;The SCC of G is the trees of dfs forest of G^T&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;minimum-spanning-tree&quot;&gt;Minimum spanning Tree&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Spanning tree:&lt;/em&gt;&lt;/strong&gt; A spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G with minimum possible number edges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Weighted graph:&lt;/em&gt;&lt;/strong&gt; A weighted graph is a graph where a number is assigned to each edge. The weights may represent costs, length and some other things.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Connected graph:&lt;/em&gt;&lt;/strong&gt; For an undirected graph, if there is a path between two vertices (v and u), then v and u are connected. In a directed graph, the path connecting v and u must be in the same direction. A graph is called connected graph if any two vertices are connected.&lt;/p&gt;

&lt;p&gt;For a spanning tree of a weighted connected graph, if the total weight is smallest among all spanning trees, then it is called minimum spanning tree.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Minimum spanning Tree problem:&lt;/em&gt;&lt;/strong&gt; Find an MST for the input graph&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Minimum spanning Forest problem:&lt;/em&gt;&lt;/strong&gt;  If the given graph is not necessarily connected, find MST for each connected component.&lt;/p&gt;

&lt;h4 id=&quot;kruskals-algorithm&quot;&gt;Kruskal’s algorithm&lt;/h4&gt;

&lt;p&gt;Input: An edge-weighted (simple, undirected, connected) graph.&lt;/p&gt;

&lt;p&gt;Output: an MST&lt;/p&gt;

&lt;p&gt;Idea: Suppose G = (V(m), E(n)). We consider the graph as the forest with no edge, each vertex is a tree. Then order the weight of edges, start adding edge in non-decreasing order. If the new edge connect two vertices on the same current formed tree, we do not add this edge. If the edge connect two nodes belongs to two trees, we add this edge.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;procedure kruskal (G)

T &amp;lt;-- \o
for each v \in V(G) do
	Define cluster C(v) &amp;lt;-- v // each vertex is a tree
sort edges in E(G) into non-decreasing weight order
for each edge e_i = (u,v) /in E(G) do
	if C(u) != C(v) then
		T &amp;lt;-- T /cup {e_i}
		merge clusters C(u) and C(v)
return T
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;correctness-of-kruskal&quot;&gt;Correctness of Kruskal&lt;/h5&gt;

&lt;p&gt;We prove that after every iteration of the second for loop, where we have examined i edges and have a partial solution built into T, called it T_i, there is a final MST T_opt which has all these edges of T_i and all other edges the T_opt has but T_i does not are in the edges we have not examined yet. i.e., &lt;script type=&quot;math/tex&quot;&gt;T_i  \subseteq T_(opt) \subseteq T_i \cup {e_(i+1), e_(i+2), ... , e_m}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The base case is trivial. And once we prove it for the last iteration e_m, it implies the solution is MST.&lt;/p&gt;

&lt;p&gt;So we now consider the induction step after we have T_i satisfying the ih and examine edge e_i+1. If we do not add e_i+1, then T_i = T_i+1. In this case, T_opt do not have e_i+1 too for sure. If we create a cycle with e_i+1, then T_opt cannot have e_i+1 too because all edges of T_i are in T_opt and e_i+1 cannot belong to T_opt.&lt;/p&gt;

&lt;p&gt;If we add e_i+1 to T_i. There are two cases.&lt;/p&gt;

&lt;p&gt;Case 1: T_opt contains it. So our T_i+1 is consistent with optimum solution.&lt;/p&gt;

&lt;p&gt;Case 2: T_opt does not contain it. In this case, there must be a cycle in T_opt + e_i+1. This cycle contains at least one edge e_j that is not in T_i. e_j must be in {e_i+1, … , e_m}. The reason is if e_j is in T_i, we will not add e_i+1 and in this case, e_j can only be in the remaining case we have not tested. To form a cycle, e_i+1 must be different from e_j, so j &amp;gt;= i+2. So w(e_j) &amp;gt;= w(e_i+1). So T_opt - e_j + e_i+1 is also a MST.&lt;/p&gt;

&lt;h5 id=&quot;running-time&quot;&gt;Running time&lt;/h5&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Each cluster contains the unordered linked list of vertices. Each vertex keeps the index which cluster it belongs to. At first, there are n clusters. To merge two clusters, we need to change the indexes of each vertex in the smaller cluster. So the merge step costs O(min{&lt;/td&gt;
      &lt;td&gt;C(u)&lt;/td&gt;
      &lt;td&gt;,&lt;/td&gt;
      &lt;td&gt;C(v)&lt;/td&gt;
      &lt;td&gt;}) time.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After merging two clusters, the size of the total is at least double larger than the original smaller cluster. So for each vertex, we need at most log(n) time to finish its merge and in total, there are at most nlog(n) time for merge of all vertices.&lt;/p&gt;

&lt;p&gt;For the sort part, it costs O(m&lt;em&gt;log(m)) and m &amp;lt;= n(n-1) for an undirected graph. So O(m&lt;/em&gt;log(m)) = O(mlog(n^2)) = O(2mlog(n)) = O(mlog(n)). So the total time is O((m+n)log(n)).&lt;/p&gt;

&lt;h4 id=&quot;prims-algorithm&quot;&gt;Prim’s algorithm&lt;/h4&gt;

&lt;p&gt;Idea: Initialize the set V_new = {x}. x is any vertex in the set V and seen as the start vertex. E_new = {}. Repeat the following steps until V_new = V. Find the smallest weight edge in E, the edge must have a vertex u which is in V_new and a vertex v which is not in V_new. Add v into V_new and add (u, v) into E_new. Return the V_new and E_new.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;procedure primMST(G) // G = (V, E)
S = {s}
A = empty set
while (|S| &amp;lt; |V|) do
	find a minimum weight edge e = (u,v), u \in S and v \notin S
	S = S \cup {v}
	A = A \cup {(u,v)}
return A
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;correctness&quot;&gt;Correctness&lt;/h5&gt;

&lt;p&gt;A_i: The set of edges A in the algorithm  after i iterations of the while loop.&lt;/p&gt;

&lt;p&gt;We can state the claim in the form, for any 0 &amp;lt;= i &amp;lt;= n-1, there exists T_opt such that &lt;script type=&quot;math/tex&quot;&gt;A_i \subseteq T_opt \subseteq A_i \cup E \setminus  A_i&lt;/script&gt;. This similar proof cannot provide any convenience.&lt;/p&gt;

&lt;p&gt;We need to prove, For all i from 0 to n-1, there exists MST T_opt such that A_i \subset T_opt. Notice that A_n-1 is a spanning tree of G, if we prove A_n-1 \subset T_opt, we prove A_n-1 is the MST.&lt;/p&gt;

&lt;p&gt;Base step, Ind step, assume there is a T_opt such that A_i \subset T_opt, we need to show there is a &lt;script type=&quot;math/tex&quot;&gt;T'_{opt}&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;A_i \cup \{new e\} \subset T'_{opt}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If e \in &lt;script type=&quot;math/tex&quot;&gt;T_{opt}&lt;/script&gt;, we are done. If &lt;script type=&quot;math/tex&quot;&gt;e \notin T_{opt}&lt;/script&gt;, there must be an edge e’ = (u’, v’) in &lt;script type=&quot;math/tex&quot;&gt;T_{opt}&lt;/script&gt; such that crosses the (S, &lt;script type=&quot;math/tex&quot;&gt;\bar S&lt;/script&gt;). Clearly, w(e) &amp;lt;= w(e’) because e’ must not in A_i and until A_i, we have already looked for all smallest edges. In this case, &lt;script type=&quot;math/tex&quot;&gt;e' \in T_{opt}&lt;/script&gt;, then we can remove e’ and add e since e is smaller, &lt;script type=&quot;math/tex&quot;&gt;T'_{opt} = T_{opt} \setminus {e'} \cup {e}&lt;/script&gt;&lt;/p&gt;

&lt;h5 id=&quot;a-revise-of-prims-algorithm&quot;&gt;A revise of Prim’s algorithm&lt;/h5&gt;

&lt;p&gt;Idea: For each node \notin S, keep track of the min edge that connects it to S. Update the information only for the neighbors of the node that is currently being added to S. Uses a priority queue Q on \bar S.&lt;/p&gt;

&lt;p&gt;Pseudocode:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;procedure primMST(G)
for each v \in V(G) do
	v.key = \infinity
	v.predec = NIL
s.key = 0
while (Q != emptyset) do
	u = ExtractMin(Q)
	for each v neighbor of u do
			if (v \in Q and w(u,v) &amp;lt; v.key) then
				v.predec = u
				decrease-key(Q,v,w(u,v))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Time: ExtrachMin(Q) costs logn and while loop will iterate n times, so it costs nlog(n) in total. The second for loop costs mlogn.&lt;/p&gt;

&lt;h5 id=&quot;a-theorem-from-prim-alg&quot;&gt;A theorem from prim alg:&lt;/h5&gt;

&lt;p&gt;Let T be a spanning tree of G, T is a MST iff each edge is the min-edge crossing to cut it induces.&lt;/p&gt;

&lt;p&gt;The cut is the connection edge of two separate parts. If the edge is cut, the tree becomes two trees.&lt;/p&gt;

&lt;h4 id=&quot;the-shortest-path-problem&quot;&gt;The shortest path problem&lt;/h4&gt;

&lt;p&gt;Def of shortest path problem: find the shortest path in an edge-weighted graph connecting two vertices s and t.&lt;/p&gt;

&lt;p&gt;Def of SSSP single source shortest path problem: Given an edge-weighted graph G and a source s, find out for each vertex v \in V(G) a shortest paths from s to v.&lt;/p&gt;

&lt;h5 id=&quot;some-properties&quot;&gt;Some properties:&lt;/h5&gt;

&lt;p&gt;The sub-path optimality: If we find some shortest path from u_0 to u_k, then some path u_i to u_j is also the shortest path between u_i and u_j.&lt;/p&gt;

&lt;p&gt;The shortest path is good if there are negative weight directed, if it is undirected the negative weight is meaningless. We assume no negative cycles.&lt;/p&gt;

&lt;p&gt;For any u, we have d(u, u) = 0&lt;/p&gt;

&lt;p&gt;For any u, v, we have d(u, v) = d(v, u) (if G is undirected)&lt;/p&gt;

&lt;p&gt;For any u, v, w we have d(u, w) &amp;lt;= d(u, v) + d(v, w)&lt;/p&gt;

&lt;h4 id=&quot;dijkstras-sssp-algorithm&quot;&gt;Dijkstra’s SSSP algorithm&lt;/h4&gt;

&lt;p&gt;For graphs with non-negative weights, idea:&lt;/p&gt;

&lt;p&gt;Maintains a set S of vertices for which we know the shortest path. Initially, S = {s}, at the end: S = V. By the greedy solution, we should choose the node in \bar S = V \setminus S with the minimum dist.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;procedure dijkstra(G, w,s) //G = (V,E)
for each v do
	v.dist &amp;lt;-- /infinity
	v.predec &amp;lt;-- NIL
s.dist &amp;lt;-- 0
Build Min-priority-Queue Q on all nodes, k = dist
while (Q != empty) do
	u &amp;lt;-- EtractMin(Q)
	for each v neighbor of u
		if (v.dist &amp;gt; u.dist + w(u,v)) then
			v.dist &amp;lt;-- u.dist + w(u,v)
			v.predec &amp;lt;-- u
			decrease-key(Q, v, v.dist)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sat, 11 Apr 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/04/11/Intro-Graph/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/11/Intro-Graph/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Mda Game Analysis</title>
        <description>
</description>
        <pubDate>Fri, 27 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/27/MDA-game-Analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/27/MDA-game-Analysis/</guid>
        
        
      </item>
    
      <item>
        <title>Temp TODO</title>
        <description>&lt;p&gt;最近挖了很多坑，做一个短期和中期的todo list。&lt;/p&gt;

&lt;p&gt;短期(in one week):&lt;/p&gt;

&lt;p&gt;review alg dp&lt;/p&gt;

&lt;p&gt;game sound effect adjust&lt;/p&gt;

&lt;p&gt;finish html and CSS&lt;/p&gt;

&lt;p&gt;finish 252 homework&lt;/p&gt;

&lt;p&gt;252 lab&lt;/p&gt;

&lt;p&gt;296 homework&lt;/p&gt;

&lt;p&gt;cs 231 lec 1-5&lt;/p&gt;

&lt;p&gt;中期(in the summer holiday):&lt;/p&gt;

&lt;p&gt;research&lt;/p&gt;

&lt;p&gt;web development(js, 框架)&lt;/p&gt;

&lt;p&gt;cs 231 全部&lt;/p&gt;

</description>
        <pubDate>Wed, 25 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/25/temp-todo/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/25/temp-todo/</guid>
        
        <category>TODO list</category>
        
        
      </item>
    
      <item>
        <title>Some dp problems</title>
        <description>&lt;h4 id=&quot;matrix-chain-multiplication-矩阵链乘积&quot;&gt;Matrix chain multiplication (矩阵链乘积)&lt;/h4&gt;

&lt;p&gt;Suppose we can several input matrices &lt;script type=&quot;math/tex&quot;&gt;A_1&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0*d_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;A_2&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_1*d_2&lt;/script&gt;, …, &lt;script type=&quot;math/tex&quot;&gt;A_n&lt;/script&gt;  with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_{n-1}* d_n&lt;/script&gt;. And we want to get the output&lt;script type=&quot;math/tex&quot;&gt;A_1* A_2*... * A_n&lt;/script&gt; and using the minimum number of scalar multiplication (the multiplication between numbers). Suppose we want to multiply &lt;script type=&quot;math/tex&quot;&gt;A_1 and A_2&lt;/script&gt;, we will have &lt;script type=&quot;math/tex&quot;&gt;d_0*d_1*d_2&lt;/script&gt; scale multiplications.&lt;/p&gt;

&lt;p&gt;Note: When &lt;script type=&quot;math/tex&quot;&gt;A_1&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0, d_1&lt;/script&gt; times&lt;script type=&quot;math/tex&quot;&gt;A_2&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_1, d_2&lt;/script&gt;, the result is the matrix with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0, d_2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So here is an example of different combinations of matrix multiplication: n=4, (d0,d1,d2,d3,d4) = (5,2,6,4,3)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;((A1 *A2) *A3) * A4 \ is\ d0 *d1 *d2 + d0 * d2 *d3 + d0 *d3 * d4 = 5 * 2 * 6 + 5 * 6 * 4 + 5 * 4 * 3 = 240&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(A1 × (A2 × A3)) × A4 \ is \ d_1 * d_2 * d_3 + d_0 * d_1 * d_3 + d_0 * d_3 * d_4 = 148&lt;/script&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;h5 id=&quot;recursion-solution&quot;&gt;Recursion solution&lt;/h5&gt;

&lt;p&gt;Consider (A_1 * … * A_i) * (A_i+1 * … * A_n), the multiplication number is d_0 * d_i * d_n. In the next recursion, we need to find the least-costly way to multiply first i and the last n-i matrixes. For i, there are n-1 positions to stay, i.e. 1,2, …, n-1.&lt;/p&gt;

&lt;p&gt;So we get the equation &lt;script type=&quot;math/tex&quot;&gt;R(1, n) = min_{1\leq i \leq n-1} (d_0 * d_i * d_n + R(1,i) + R(i+1, n))&lt;/script&gt;. The base case is R(i, i) for any i.&lt;/p&gt;

&lt;p&gt;The runtime is &lt;script type=&quot;math/tex&quot;&gt;\Omega ({3^n})&lt;/script&gt;&lt;/p&gt;

&lt;h5 id=&quot;dp-solution&quot;&gt;DP solution&lt;/h5&gt;

&lt;p&gt;Define M[i, j] (1 &amp;lt;= i &amp;lt;= j) is the minimal number of scalar multiplications needed to compute A_i * A_i+1 * … * A_j.&lt;/p&gt;

&lt;p&gt;Then 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{cases}
	0, 			\ if\  i=j\\
	min_{i\leq k &lt;j}(M[i,k]+M[k+1, j]+ d_{i-1}d_kd_j), \ if\ i&lt;j
	\end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/24/dp-problem/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/24/dp-problem/</guid>
        
        <category>Problem</category>
        
        
      </item>
    
      <item>
        <title>Linear Regression and polynomial regression</title>
        <description>&lt;p&gt;To learn the relationship between features and targets (x_i and y_i), we usually hypothesize the functional form of the relationship. The function can be linear or non-linear. e.g. f(X) = w0+w1x1+w2x2 and f(X) = a + bX1X2. The w0 w1 w2 and a and b are the parameters need to be learned.&lt;/p&gt;

&lt;p&gt;In particular, the linear function is modeled as a linear combination of features (x) and parameters (w), i.e. f(X) = w0 + w1x1 + … + wdxd = sum(wjxj) = x^T w. We can extend x as (x0=1, x1, x2 …  xd). This form allows us use the dot product. Finding the best parameters (w0 to wd) isreferred to as the linear regression problem.&lt;/p&gt;

&lt;h4 id=&quot;formalizing-the-maximum-likelihood-problem&quot;&gt;Formalizing the maximum likelihood problem&lt;/h4&gt;

&lt;p&gt;Assume the observed data set D is a product of a data generating process in which n data points were drawn independently and according to the same distribution p(x). Assume the target variable Y has the linear relationship with input variable X, existing some error term  Ɛ. And the Ɛ follows the Gaussian distribution where the mean is zero. &lt;script type=&quot;math/tex&quot;&gt;Y =  \sum_{j=0}^{d} w_j X_j + Ɛ&lt;/script&gt; . X0=1 is the intercept term. The assumption of normality for the error term is reasonable because of the central limit theorem.
&lt;script type=&quot;math/tex&quot;&gt;p(y|x,w) = (1/\sqrt(2\pi\sigma^2))exp(-(y-x^Tw)^2/2\sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To implement the MLE problem,
&lt;script type=&quot;math/tex&quot;&gt;W_{MLE}=argmin_{w\in F} - \sum_{i=1}^n ln(p(y_i|x_i,w))
= argmin_{w\in F} \sum_{i=1}^n ln\sqrt{2\pi\sigma^2}+\sum_{i=1}^n (y_i-x_i^Tw)^2/2\sigma^2&lt;/script&gt;
Only the second part is related with w, so
&lt;script type=&quot;math/tex&quot;&gt;= argmin_{w\in F} \frac{1} {2\sigma^2}*\sum_{i=1}^n (y_i-x_i^Tw)^2 \ = argmin_{w\in F}\sum_{i=1}^n (y_i-x_i^Tw)^2&lt;/script&gt;
To find the w, we need to minimize the difference between real y_i and &lt;script type=&quot;math/tex&quot;&gt;\hat{y_i}&lt;/script&gt; which is &lt;script type=&quot;math/tex&quot;&gt;x_i^T w&lt;/script&gt;. So this follows the intuition.&lt;/p&gt;

&lt;p&gt;E.g. Consider data set {(1,1.2), (2, 2.3), (3, 2.3), (4, 3.3)}. We want to find the max likelihood coefficients for f(x) = w_0 + w_1x. Note that there is one column in X which is ones because that can make w_0 times 1.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X= \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1&amp;3 \\ 1&amp;4\\ \end{bmatrix}     w = \begin{bmatrix} w_0  \\ w_1 \\ \end{bmatrix}   y =  \begin{bmatrix} 1.2  \\ 2.3 \\ 2.3 \\ 3.3 \\ \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the next section, we can use some new equation to get the answer of w.&lt;/p&gt;

&lt;h4 id=&quot;linear-regression-solution&quot;&gt;Linear Regression solution&lt;/h4&gt;

&lt;p&gt;We define the squared errors &lt;script type=&quot;math/tex&quot;&gt;c_i(W) = (f(x_i) - y_i)^2 = \frac{1} {2} (x_i^Tw - y_i)^2&lt;/script&gt;. And &lt;script type=&quot;math/tex&quot;&gt;c(w) = \frac {1} {n} \sum{i=1}^n c_i(w)&lt;/script&gt;. In this case, we can get the average squared error instead of a cumulative error. And this is same in the MLE problem because 1/n does not affect the w’s effect in the equation.&lt;/p&gt;

&lt;p&gt;To solve the MLE problem, we usually get the gradient of the equation, &lt;script type=&quot;math/tex&quot;&gt;\nabla c(w) = \frac {1} {n} \sum_{i=1}^n \nabla c_i(w)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\LR-solution.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the result for c_i and w_j. So for the c(w), we need to obtain the system of equations with d+1 variables and d+1 equations.
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i0}=0\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i1}=0\\...\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{id}=0&lt;/script&gt;
Which is same as 
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n} \sum_{i=1}^n (\vec{x_i^T}w - y_i)\vec{x_i} = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;When we turn it as a linear algebra problem, we can define &lt;script type=&quot;math/tex&quot;&gt;\vec {A}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \vec{x_i^T}  \in R^{(d+1)*(d+1)}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec {b}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \cdot y_i \in R^{d+1}&lt;/script&gt;

&lt;p&gt;So &lt;script type=&quot;math/tex&quot;&gt;\vec {A} \cdot \vec {w} = \vec {b}&lt;/script&gt;. If A is invertible, then &lt;script type=&quot;math/tex&quot;&gt;\vec {w} =\vec {A^{-1}} \vec {b}&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;solve-lr-using-gradient-descent&quot;&gt;Solve LR using gradient descent&lt;/h5&gt;

&lt;p&gt;In     gradient descent, we would initialize the weights $\vec {w}$ at some random initialization and iteratively update $\vec {w}$ until we reach a point where the gradient is near 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\in-post\Rl\rl_gd.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Analysis of time cost:&lt;/p&gt;

&lt;p&gt;Using gradient descent can be more quick than the linear algebra solution, each gradient descent update costs O(nd) and the solution to the linear system costs O(d^3 + d^2n). Constructing A costs O(d^2n) and inversing matrix using O(d^3). For gradient descent, the k iteration costs O(ndk).&lt;/p&gt;

&lt;p&gt;To improve the efficient, we use stochastic gradient descent.&lt;/p&gt;

&lt;h4 id=&quot;stochastic-gradient-descent-and-handling-big-data-sets&quot;&gt;Stochastic gradient descent and handling Big Data sets&lt;/h4&gt;

&lt;p&gt;In stochastic approximation, we only use one sample or a small mini-batch of b samples (e.g., b = 32). For example, if we only use one sample, we can use this point to update the gradient:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
w_{t+1} &lt;-- w_{t} - n_t(\vec{x_i^T}-y_i)\vec{x_i} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/18/Regression/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/18/Regression/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Formalizing Parameter Estimation</title>
        <description>&lt;p&gt;We want to find some function or model which satisfied some requirements. First, the model should be able to have performance on the previously unseen data should not deteriorate(change) once the new data is presented. Second, function must be able to include information about the model space from which it is selected and process of selecting a model should be able to accept training “advice” from an analyst. Finally, when large amounts of data are available learning algorithms must be able to provide solutions in reasonable time given the resource.&lt;/p&gt;

&lt;p&gt;In this section, we can use MAP and MLE to estimate some distribution’s parameters (mean μ and SD σ for normal distribution λ for Poisson distribution.)&lt;/p&gt;

&lt;h4 id=&quot;map-and-maximum-likelihood-estimationmle&quot;&gt;MAP and Maximum likelihood Estimation(MLE)&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Imagine there is a dataset of observations D={x&lt;sub&gt;i&lt;/sub&gt;} (i from 1 to n). It has an unknown but true distribution p*. However, if you know the distribution is in a set of possible distribution, F, F is called the hypothesis space or function class. For example, F can be the family of all univariate Gaussian distributions. In this family, the mean μ* and SD σ* can be different. F = {N(μ, σ^2)&lt;/td&gt;
      &lt;td&gt;for any μ ∈ R and σ ∈ R+}. While the true distribution has parameters μ*, σ*, we need to find μ and σ as close to the true ones as possible.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The idea of MAP estimation is to find the most probable model for the observed data. Given the dataset D, the solution of MAP is f&lt;sub&gt;MAP&lt;/sub&gt; = argmax&lt;sub&gt; f∈F&lt;/sub&gt; p(f&lt;/td&gt;
      &lt;td&gt;D).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(f&lt;/td&gt;
      &lt;td&gt;D) is called the posterior distribution of the model given the data. MAP estimate is exactly the most probable model.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To calculate the posterior distribution we apply he Bayes rules:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(f&lt;/td&gt;
      &lt;td&gt;D) = p(D&lt;/td&gt;
      &lt;td&gt;f)p(f)/p(D)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(D&lt;/td&gt;
      &lt;td&gt;f) is called the likelihood function, p(f) is the prior distribution of the model. p(D) is the marginal distribution of the data.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To compute p(D), we need to sum all situations with different f ∈ F. p(D) = sum(p(D&lt;/td&gt;
      &lt;td&gt;f)*p(f)).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;max &lt;sub&gt;fF&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f)/p(D) = 1/p(D) * max &lt;sub&gt;f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f). P(D) is not related with the MAP solution. Now f&lt;sub&gt;MAP&lt;/sub&gt; = argmax&lt;sub&gt; f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f). If in some situations, we cannot prefer one model and have the same p(f) for all f in the hypothesis space. Then MAP becomes MLE. f&lt;sub&gt;MLE&lt;/sub&gt; = argmax &lt;sub&gt;f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f).  (The p(f) is not considered). So MLE can be seen as a special case of MAP.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;EX: Suppose data set D = {2,5,9,5,4,8} is an i.i.d. sample from a Poisson distribution with a fixed but unknown parameter λ&lt;sub&gt;0&lt;/sub&gt;. Find the MLE solution of λ&lt;sub&gt;0&lt;/sub&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;λ&lt;sub&gt;MLE&lt;/sub&gt; = argmax p(D&lt;/td&gt;
      &lt;td&gt;λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(D&lt;/td&gt;
      &lt;td&gt;λ) = p({x&lt;sub&gt;i&lt;/sub&gt;}&lt;/td&gt;
      &lt;td&gt;λ) = p(x&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)p(x&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)*…*p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To find a λ to make the p maximum. ln(p(D&lt;/td&gt;
      &lt;td&gt;λ)) = ln(p(x&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)p(x&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)*…*p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;= sum(ln(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In Poisson distribution p(x&lt;/td&gt;
      &lt;td&gt;λ)=λ^x*e^(-λ)/x!, ln(p(x&lt;/td&gt;
      &lt;td&gt;λ)) = ln(λ^x)+ln(e^(-λ))-ln(x!) = x*ln(λ)-λ-ln(x!)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;sum(ln(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ))) = sum(x&lt;sub&gt;i&lt;/sub&gt;ln(λ)) - nλ - sum(ln(x!))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;By computing the first derivative, we can compute the λ which makes ln.. max.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;d ln(p(D&lt;/td&gt;
      &lt;td&gt;λ))/d λ = (1/λ)sum(x&lt;sub&gt;i&lt;/sub&gt;) - n. In this case, n = 6. λ=sum(x&lt;sub&gt;i&lt;/sub&gt;)/n&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Ex of MAP: D = {2,5,9,5,4,8}. i.i.d. sample from Poisson with mean λ&lt;sub&gt;0&lt;/sub&gt;. We need to calculate the MAP estimate of λ&lt;sub&gt;0&lt;/sub&gt;. Some other information is given. Suppose the prior knowledge about λ&lt;sub&gt;0&lt;/sub&gt; can be expressed using a gamma distribution with k=3 and θ=1. Find the MAP of λ&lt;sub&gt;0&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;By gamma distribution, p(λ) = λ^(k-1)e^(-λ/θ)/θgamma&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;By Poisson distribution, p(x&lt;/td&gt;
      &lt;td&gt;λ) = λ^x*e^-λ/x!&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;λ&lt;sub&gt;MAP&lt;/sub&gt; = argmax &lt;sub&gt;λ∈(0, ∞)&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ). Taking log to simplify.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ln p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ) = ln(p(D&lt;/td&gt;
      &lt;td&gt;λ)) + ln(p(λ)) = ln sum(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)) + ln p(λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ln p(λ) = ln λ^(k-1)e^(-λ/e)/θgamma(k) = ln  λ^(k-1) + ln e^(-λ/e) - ln θgamma(k) = (k-1) ln λ - (λ/θ) - ln θgamma(k)&lt;/p&gt;

&lt;p&gt;sum(ln (λ^x&lt;sub&gt;i&lt;/sub&gt;&lt;em&gt;e^-λ/x&lt;sub&gt;i&lt;/sub&gt;)) = sum(x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;ln λ - λ - ln x&lt;sub&gt;i&lt;/sub&gt;) = ln λ sum(x&lt;sub&gt;i&lt;/sub&gt;) - nλ - ln sum(x&lt;sub&gt;i&lt;/sub&gt;)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Taking derivate: d(ln p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ)) / d λ= sum(x&lt;sub&gt;i&lt;/sub&gt;)/λ - n + (k-1)/λ - 1/θ = 0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;sum(x&lt;sub&gt;i&lt;/sub&gt;) + (k-1) = (n + 1/θ)λ&lt;/p&gt;

&lt;p&gt;So λ&lt;sub&gt;MAP&lt;/sub&gt; = (k-1 + sum(x&lt;sub&gt;i&lt;/sub&gt;))/(n + 1/θ) = (3-1+(2+5+9+5+4+8))/(6+1) = 35/7 = 5&lt;/p&gt;

&lt;h5 id=&quot;the-effect-of-the-amount-of-data&quot;&gt;The effect of the amount of data&lt;/h5&gt;

&lt;p&gt;When the data amount is large, the result of MAP becomes closer to MLE. The importance of prior is reduced.&lt;/p&gt;

&lt;p&gt;An example of Poisson distribution: s&lt;sub&gt;n&lt;/sub&gt; = sum(x&lt;sub&gt;i&lt;/sub&gt;), which is from the random variable S&lt;sub&gt;n&lt;/sub&gt; = sum(X&lt;sub&gt;i&lt;/sub&gt;) . And suppose s&lt;sub&gt;n&lt;/sub&gt;/n^2 –&amp;gt; ∞ when n –&amp;gt; ∞.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\web1111\xichen1.github.io\img\MLE-MAP.JPG&quot; alt=&quot;MLE-MAP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ex: Let D={x&lt;sub&gt;i&lt;/sub&gt;}, i from 1 to n. It is an i.i.d. sample from a normal distribution. To find the MLE of the parameters.&lt;/p&gt;

&lt;p&gt;By taking partial derivatives of μ, σ, we get the result μ&lt;sub&gt;MLE&lt;/sub&gt; = 1/n(sum(x&lt;sub&gt;i&lt;/sub&gt;)), σ&lt;sub&gt;MLE&lt;/sub&gt; = 1/n(sum(x&lt;sub&gt;i&lt;/sub&gt;-μ&lt;sub&gt;MLE&lt;/sub&gt;)^2).&lt;/p&gt;

&lt;h4 id=&quot;bayesian-estimation&quot;&gt;Bayesian estimation&lt;/h4&gt;

&lt;p&gt;Maximum a posterior and maximum likelihood approaches report the solution that is consistent with the mode of the posterior distribution and the likelihood function. But the MLE and MAP cannot deal with the skewed distributions and multimodal distributions. Bayesian can!&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Formalizing-Parmeter-Estimation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Formalizing-Parmeter-Estimation/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Proof of Alg.</title>
        <description>&lt;p&gt;If the code is written using recursion, prove correctness using induction.&lt;/p&gt;

&lt;h5 id=&quot;loop-invariant&quot;&gt;Loop invariant&lt;/h5&gt;

&lt;p&gt;For code written using loops, prove correctness by the loop invariant method.&lt;/p&gt;

&lt;p&gt;A loop-invariant is an assertion about the state of the code that is always true at the beginning of each loop-iteration. Two steps of using loop-invariant: 1. identify the LI. 2. prove the LI for initialization, maintenance, termination#1 (stop eventually), termination#2 (right result).&lt;/p&gt;

&lt;p&gt;One example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;FindSum(A,n)
	sum &amp;lt;-- A[1]
	j &amp;lt;-- 2
	while (j&amp;lt;=n)
		sum &amp;lt;-- sum + A[j]
		j &amp;lt;-- j+1
	return sum
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The LI is “At the beginning of each loop iteration, sum = (j-1)sum(i=1)A[i]”.&lt;/p&gt;

&lt;h6 id=&quot;the-proof-of-li&quot;&gt;The proof of LI&lt;/h6&gt;

&lt;p&gt;Initially: Before the loop begins sum = A[1] = A[1,…,(2-1)]&lt;/p&gt;

&lt;p&gt;Maintenance: Suppose that at the beginning of iteration j, sum=A[1]+A[2]+…+A[j-1]. Then at the beginning of iteration j+1. Sum&lt;sub&gt;after&lt;/sub&gt;=sum&lt;sub&gt;before&lt;/sub&gt;+A[j] = A[1]+A[2]+..+A[j] = A[1]+A[2]+..+A[j+1-1].&lt;/p&gt;

&lt;p&gt;Termination #1: The loop terminates as we only increase j, so eventually j&amp;gt;n.&lt;/p&gt;

&lt;p&gt;Termination #2: When the while-loop terminates, j=n+1, in that case LI implies sum = A[1]+…+A[n].&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Proof-of-Alg/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Proof-of-Alg/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note heap</title>
        <description>&lt;p&gt;Some alg and time complexity of heap:&lt;/p&gt;

&lt;h5 id=&quot;max-heapify---θlogn---θh&quot;&gt;Max-heapify =  Θ(logn) =  Θ(h)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Max-Heapify(A,i)
		**pre-conditition: tree rooted at A[i] is and almost-heap
	lc &amp;lt;-- leftchild(i)
	rc &amp;lt;-- rightchild(i)
	largest &amp;lt;-- i
	if (lc &amp;lt;= heapsize(A) and A[lc] &amp;gt; A[largest]) then
		largest &amp;lt;-- lc
	if (rc &amp;lt;= heapsize(A) and A[rc] &amp;gt; A[largest]) then
		largest &amp;lt;-- rc
	if (largest != i) then
		exchange A[i] &amp;lt;--&amp;gt; A[largest]
		Max-heapify(A,largest)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The WC is from the top of the heap to the bottom of the heap. Every time it costs Θ(1) and in total it is h*Θ(1) = Θ(h) = Θ(logn).&lt;/p&gt;

&lt;h5 id=&quot;build-max-heap---θn&quot;&gt;Build-Max-Heap =  Θ(n)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Build-Max-Heap(A)
		** turn an array into a heap
	heapsize(A) &amp;lt;-- length[A]
	for (i &amp;lt;-- floor(length[A]/2) downto 1) do
		Max-heapify(A,i)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;heap-sort---θnlogn&quot;&gt;Heap sort =  Θ(nlogn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-Sort(A,n)
		** sort an array using a heap
	Build-Max-Heap(A)
	heapsize &amp;lt;-- n
	while (heapsize &amp;gt; 1) do
		exchange A[1] &amp;lt;--&amp;gt; A[heapsize]
		heapsize &amp;lt;-- heapsize - 1
		Max-Heapify(A[1...heapsize],1)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For each position i = n, n-1, n-2, … , 2, Max-Heapify takes O(logn). Totally, it costs Θ(nlogn).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\Capture.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;priority-queue&quot;&gt;Priority Queue&lt;/h4&gt;

&lt;h5 id=&quot;initializea--θn&quot;&gt;Initialize(A) = Θ(n)&lt;/h5&gt;

&lt;p&gt;Building a heap&lt;/p&gt;

&lt;h5 id=&quot;extract-maximuma--θlogn&quot;&gt;Extract-Maximum(A) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-Extract-Max(A)
		precondiction: A is not empty
	max &amp;lt;-- A[1]
	A[1] &amp;lt;-- A[heapsize[A]]
	heapsize[A] &amp;lt;-- heapsize[A] - 1
	if (heapsize[A]&amp;gt;0) then
		Max-Heapify(A,1)
	return max
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The exchange costs Θ(1) and max heapify costs Θ(logn).&lt;/p&gt;

&lt;h5 id=&quot;heap-increase-key-a-i-key--θlogn&quot;&gt;Heap-Increase-Key (A, i, key) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-increase-Key(A, i, key)
		** precondiction: key &amp;gt;= A[i]
	A[i] &amp;lt;-- key
	while (i&amp;gt;1 and A[parent(i)]&amp;gt;A[i]) do
		exchange A[i] &amp;lt;--&amp;gt; A[parent(i)]
		i &amp;lt;-- Parent(i)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the bottom to the top of the heap, it takes Θ(h) = Θ(logn)&lt;/p&gt;

&lt;h5 id=&quot;insert-key-a-new_key--θlogn&quot;&gt;Insert-Key (A, new_key) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Insert-Key(A,new_key)
	heapsize[A] &amp;lt;-- heapsize[A]+1
	A[heapsize[A]] &amp;lt;-- $$-/infty$$
	Heap-Increase-key(A, heapsize[A], key)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Note-of-heap/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Note-of-heap/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note QS</title>
        <description>&lt;h5 id=&quot;the-idea-of-quick-sort&quot;&gt;The idea of quick sort&lt;/h5&gt;

&lt;p&gt;​	– pick a pivot and compare it to all others&lt;/p&gt;

&lt;p&gt;​	– Rearrange A to be: [elements&amp;lt;= pivot, pivot, elements &amp;gt; pivot]&lt;/p&gt;

&lt;p&gt;​	– Recursively sort the subarrays of before and after elements&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;QuickSort(A,p,r)
		** sort the subarray A[p,...,r]
	if (p&amp;lt;r) then
		q &amp;lt;-- Partition(A,p,r)
			** Partition returns q such that
			** 1. A[q] is the pivot
			** 2. All elements &amp;lt;= pivot appear in A[p,...,q-1]
			** 3. All elements &amp;gt; pivot appear in A[q+1,...,r]
		QuickSort(A,p,q-1)
		QuickSort(A,q+1,r)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Partition(A,p,r)
	** last element, A[r], is the pivot key picked of the partition
	pivot &amp;lt;-- A[r]
	i &amp;lt;-- p-1
	for (j from p to r-1) do
		if (A[j] &amp;lt;= pivot) then
			i &amp;lt;-- i+1
			exchange A[i] &amp;lt;--&amp;gt; A[j]
	exchange A[i+1] &amp;lt;--&amp;gt; A[r]
	return i+1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;quick-sort-running-time&quot;&gt;Quick Sort running time&lt;/h5&gt;

&lt;p&gt;In each recurrence, there is n-1 Key comparisons because the pivot is compared with other n-1 keys.&lt;/p&gt;

&lt;p&gt;​			0,                                   n&amp;lt;=1&lt;/p&gt;

&lt;p&gt;T(n) =&lt;/p&gt;

&lt;p&gt;​			T(n&lt;sub&gt;1&lt;/sub&gt;)+T(n-1-n&lt;sub&gt;1&lt;/sub&gt;)+(n-1), n&amp;gt;=2&lt;/p&gt;

&lt;p&gt;n&lt;sub&gt;1&lt;/sub&gt; + 1 is the index of the pivot. So quick sort needs&lt;/p&gt;

&lt;p&gt;T(All element before pivot)  + T(All element after pivot) + (n-1)&lt;/p&gt;

&lt;p&gt;Notice that 0 &amp;lt;= n&lt;sub&gt;1&lt;/sub&gt; &amp;lt;= n-1. Because the pivot can be any element from A[1] to A[n].&lt;/p&gt;

&lt;h6 id=&quot;worst-case&quot;&gt;Worst case&lt;/h6&gt;

&lt;p&gt;When n1 is zero/n-1, the number of KC becomes T(0) + T(n-1) + (n-1). In the next level, the KC becomes n-2.&lt;/p&gt;

&lt;p&gt;T(n) = T(n-1) + (n-1)&lt;/p&gt;

&lt;p&gt;​		= T(n-1) + n-1 = T(n-2) + n-2 + n-1 = … = T(1) + 1 + 2 + 3 + … + n-1 = n(n-1)/2&lt;/p&gt;

&lt;p&gt;So T(n) =  Θ(n^2)&lt;/p&gt;

&lt;h6 id=&quot;almost-worst-case&quot;&gt;Almost worst case&lt;/h6&gt;

&lt;p&gt;When n&lt;sub&gt;1&lt;/sub&gt; is n-2 or 1, T(n) = T(1)+T(n-2)+(n-1)&lt;/p&gt;

&lt;p&gt;​											= T(n-2)+(n-1) = T(n-4)+(n-3)+(n-1) = T(1) + 1 + 3 +…+ (n-1)&lt;/p&gt;

&lt;p&gt;T(n) = (1+(n-1))n/4=n^2/4 = Θ(n^2)&lt;/p&gt;

&lt;h6 id=&quot;best-case&quot;&gt;Best case&lt;/h6&gt;

&lt;p&gt;If every time we can choose a pivot which makes # before and # after are same, then we can save all wasted KC.&lt;/p&gt;

&lt;p&gt;T(n) = 2*T((n-1)/2)+(n-1)&lt;/p&gt;

&lt;p&gt;Solving T(n) = 2T(n/2) + (n-1). Using master theorem second case, a = b =2, n^(log &lt;sub&gt;b&lt;/sub&gt;a)=n. And f(n) = n-1 = n&lt;em&gt;((log^k)n) where k=0. So T(n) = (n&lt;/em&gt;logn).&lt;/p&gt;

&lt;h6 id=&quot;almost-best-case&quot;&gt;Almost best case&lt;/h6&gt;

&lt;p&gt;Assume in each round, # before is 3n/4 and # after is n/4. Then T(n) = T(3n/4)+T(n/4)+(n-1). Or more extreme case is T(n) = T(9n/10)+T(n/10)+(n-1).&lt;/p&gt;

&lt;p&gt;In both case, the time is O(n*logn). For any split, the running time remains to be  Θ(nlogn).&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Note-of-QS/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Note-of-QS/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Divide and Conquer</title>
        <description>&lt;h5 id=&quot;merge-sort&quot;&gt;Merge-Sort&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Merge(A, lo, mid, hi)
	**pre-condition: lo&amp;lt;=mid&amp;lt;=hi, A[lo,mid] and A[mid+1,hi] sorted
	**post-condition: A[lo,hi] sorted
	
Merge-sort(A,lo,hi)
	if lo&amp;lt;hi then
		mid = floor((lo+hi)/2)
		Merge-Sort(A,lo,mid)
		Merge-sort(A,mid+1,hi)
		Merge(A,lo,mid,hi)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let denote #KC for a list of size n. Suppose the number of keys in the list is a power of 2.&lt;/p&gt;

&lt;p&gt;T(n)=(n-1)+2*T(n/2).&lt;/p&gt;

&lt;p&gt;To solve it: 1. Iterated substitution. 2. recurrence Tree 3. Guess and test. 4 Master Theorem&lt;/p&gt;

&lt;p&gt;T(n) = 2T(n/2) + (n-1) = 2&lt;em&gt;(2&lt;/em&gt;T(n/4)+(n/2-1))+(n-1)=4T(n/4)+(n-2)+(n-1) = 4(2T(n/8)+(n/4-1))+(n-2)+(n-1) = 8T(n/8)+(n-4)+(n-2)+(n-1) = 2^k*T(n/2^k)+(n-2^(k-1))+(n-2^(k-2))+…+(n-2^0)&lt;/p&gt;

&lt;p&gt;The last one is T(1)=T(n/2^k) So let 2^k=n. T(n)=2^k&lt;em&gt;T(1) + (2^k-2^(k-1)) + (2^k-2^(k-2)) + … + (2^k-2^0)= k&lt;/em&gt;2^k-sum(2^i) (0&amp;lt;=i&amp;lt;=k-1) = k*2^k-(2^k-1)=(k-1)2^k+1.&lt;/p&gt;

&lt;p&gt;2^k=n, k=logn. T(n) = (logn-1)n+1 =  O(nlogn)&lt;/p&gt;

&lt;p&gt;To prove by induction: T(n) = (k-1)2^k+1    n = 2^k&lt;/p&gt;

&lt;p&gt;Base case: T(1) = 0. k = 0, (0-1)2^0+1 = 1-1=0&lt;/p&gt;

&lt;p&gt;Induction step. Suppose T(2^k) = (k-1)2^k+1, we need to prove that T(2^(k+1))=(k)2^(k+1)+1&lt;/p&gt;

&lt;p&gt;T(2^(k+1))=(2^(k+1)-1)+2&lt;em&gt;T(2^k)= (2^(k+1)-1)+2&lt;/em&gt;(k-1)2^k+2= 2^k+(2k-2)2^k+1=(k)*2^(k+1)+1&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Divide-and-Conquer/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Divide-and-Conquer/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
  </channel>
</rss>
