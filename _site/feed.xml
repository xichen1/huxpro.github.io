<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xichen Pan</title>
    <description>hhh</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 24 Mar 2020 15:19:36 -0600</pubDate>
    <lastBuildDate>Tue, 24 Mar 2020 15:19:36 -0600</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Some dp problems</title>
        <description>&lt;h4 id=&quot;matrix-chain-multiplication-矩阵链乘积&quot;&gt;Matrix chain multiplication (矩阵链乘积)&lt;/h4&gt;

&lt;p&gt;Suppose we can several input matrices &lt;script type=&quot;math/tex&quot;&gt;A_1&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0*d_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;A_2&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_1*d_2&lt;/script&gt;, …, &lt;script type=&quot;math/tex&quot;&gt;A_n&lt;/script&gt;  with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_{n-1}* d_n&lt;/script&gt;. And we want to get the output&lt;script type=&quot;math/tex&quot;&gt;A_1* A_2*... * A_n&lt;/script&gt; and using the minimum number of scalar multiplication (the multiplication between numbers). Suppose we want to multiply &lt;script type=&quot;math/tex&quot;&gt;A_1 and A_2&lt;/script&gt;, we will have &lt;script type=&quot;math/tex&quot;&gt;d_0*d_1*d_2&lt;/script&gt; scale multiplications.&lt;/p&gt;

&lt;p&gt;Note: When &lt;script type=&quot;math/tex&quot;&gt;A_1&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0, d_1&lt;/script&gt; times&lt;script type=&quot;math/tex&quot;&gt;A_2&lt;/script&gt; with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_1, d_2&lt;/script&gt;, the result is the matrix with dimensions &lt;script type=&quot;math/tex&quot;&gt;d_0, d_2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So here is an example of different combinations of matrix multiplication: n=4, (d0,d1,d2,d3,d4) = (5,2,6,4,3)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;((A1 *A2) *A3) * A4 \ is\ d0 *d1 *d2 + d0 * d2 *d3 + d0 *d3 * d4 = 5 * 2 * 6 + 5 * 6 * 4 + 5 * 4 * 3 = 240&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(A1 × (A2 × A3)) × A4 \ is \ d_1 * d_2 * d_3 + d_0 * d_1 * d_3 + d_0 * d_3 * d_4 = 148&lt;/script&gt;

&lt;p&gt;and so on.&lt;/p&gt;

&lt;h5 id=&quot;recursion-solution&quot;&gt;Recursion solution&lt;/h5&gt;

&lt;p&gt;Consider (A_1 * … * A_i) * (A_i+1 * … * A_n), the multiplication number is d_0 * d_i * d_n. In the next recursion, we need to find the least-costly way to multiply first i and the last n-i matrixes. For i, there are n-1 positions to stay, i.e. 1,2, …, n-1.&lt;/p&gt;

&lt;p&gt;So we get the equation &lt;script type=&quot;math/tex&quot;&gt;R(1, n) = min_{1\leq i \leq n-1} (d_0 * d_i * d_n + R(1,i) + R(i+1, n))&lt;/script&gt;. The base case is R(i, i) for any i.&lt;/p&gt;

&lt;p&gt;The runtime is &lt;script type=&quot;math/tex&quot;&gt;\Omega ({3^n})&lt;/script&gt;&lt;/p&gt;

&lt;h5 id=&quot;dp-solution&quot;&gt;DP solution&lt;/h5&gt;

&lt;p&gt;Define M[i, j] (1 &amp;lt;= i &amp;lt;= j) is the minimal number of scalar multiplications needed to compute A_i * A_i+1 * … * A_j.&lt;/p&gt;

&lt;p&gt;Then 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{cases}
	0, 			\ if\  i=j\\
	min_{i\leq k &lt;j}(M[i,k]+M[k+1, j]+ d_{i-1}d_kd_j), \ if\ i&lt;j
	\end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 24 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/24/dp-problem/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/24/dp-problem/</guid>
        
        <category>Problem</category>
        
        
      </item>
    
      <item>
        <title>Linear Regression and polynomial regression</title>
        <description>&lt;p&gt;To learn the relationship between features and targets (x_i and y_i), we usually hypothesize the functional form of the relationship. The function can be linear or non-linear. e.g. f(X) = w0+w1x1+w2x2 and f(X) = a + bX1X2. The w0 w1 w2 and a and b are the parameters need to be learned.&lt;/p&gt;

&lt;p&gt;In particular, the linear function is modeled as a linear combination of features (x) and parameters (w), i.e. f(X) = w0 + w1x1 + … + wdxd = sum(wjxj) = x^T w. We can extend x as (x0=1, x1, x2 …  xd). This form allows us use the dot product. Finding the best parameters (w0 to wd) isreferred to as the linear regression problem.&lt;/p&gt;

&lt;h4 id=&quot;formalizing-the-maximum-likelihood-problem&quot;&gt;Formalizing the maximum likelihood problem&lt;/h4&gt;

&lt;p&gt;Assume the observed data set D is a product of a data generating process in which n data points were drawn independently and according to the same distribution p(x). Assume the target variable Y has the linear relationship with input variable X, existing some error term  Ɛ. And the Ɛ follows the Gaussian distribution where the mean is zero. &lt;script type=&quot;math/tex&quot;&gt;Y =  \sum_{j=0}^{d} w_j X_j + Ɛ&lt;/script&gt; . X0=1 is the intercept term. The assumption of normality for the error term is reasonable because of the central limit theorem.
&lt;script type=&quot;math/tex&quot;&gt;p(y|x,w) = (1/\sqrt(2\pi\sigma^2))exp(-(y-x^Tw)^2/2\sigma^2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To implement the MLE problem,
&lt;script type=&quot;math/tex&quot;&gt;W_{MLE}=argmin_{w\in F} - \sum_{i=1}^n ln(p(y_i|x_i,w))
= argmin_{w\in F} \sum_{i=1}^n ln\sqrt{2\pi\sigma^2}+\sum_{i=1}^n (y_i-x_i^Tw)^2/2\sigma^2&lt;/script&gt;
Only the second part is related with w, so
&lt;script type=&quot;math/tex&quot;&gt;= argmin_{w\in F} \frac{1} {2\sigma^2}*\sum_{i=1}^n (y_i-x_i^Tw)^2 \ = argmin_{w\in F}\sum_{i=1}^n (y_i-x_i^Tw)^2&lt;/script&gt;
To find the w, we need to minimize the difference between real y_i and &lt;script type=&quot;math/tex&quot;&gt;\hat{y_i}&lt;/script&gt; which is &lt;script type=&quot;math/tex&quot;&gt;x_i^T w&lt;/script&gt;. So this follows the intuition.&lt;/p&gt;

&lt;p&gt;E.g. Consider data set {(1,1.2), (2, 2.3), (3, 2.3), (4, 3.3)}. We want to find the max likelihood coefficients for f(x) = w_0 + w_1x. Note that there is one column in X which is ones because that can make w_0 times 1.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X= \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \\ 1&amp;3 \\ 1&amp;4\\ \end{bmatrix}     w = \begin{bmatrix} w_0  \\ w_1 \\ \end{bmatrix}   y =  \begin{bmatrix} 1.2  \\ 2.3 \\ 2.3 \\ 3.3 \\ \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the next section, we can use some new equation to get the answer of w.&lt;/p&gt;

&lt;h4 id=&quot;linear-regression-solution&quot;&gt;Linear Regression solution&lt;/h4&gt;

&lt;p&gt;We define the squared errors &lt;script type=&quot;math/tex&quot;&gt;c_i(W) = (f(x_i) - y_i)^2 = \frac{1} {2} (x_i^Tw - y_i)^2&lt;/script&gt;. And &lt;script type=&quot;math/tex&quot;&gt;c(w) = \frac {1} {n} \sum{i=1}^n c_i(w)&lt;/script&gt;. In this case, we can get the average squared error instead of a cumulative error. And this is same in the MLE problem because 1/n does not affect the w’s effect in the equation.&lt;/p&gt;

&lt;p&gt;To solve the MLE problem, we usually get the gradient of the equation, &lt;script type=&quot;math/tex&quot;&gt;\nabla c(w) = \frac {1} {n} \sum_{i=1}^n \nabla c_i(w)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\LR-solution.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the result for c_i and w_j. So for the c(w), we need to obtain the system of equations with d+1 variables and d+1 equations.
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i0}=0\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i1}=0\\...\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{id}=0&lt;/script&gt;
Which is same as 
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{n} \sum_{i=1}^n (\vec{x_i^T}w - y_i)\vec{x_i} = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;When we turn it as a linear algebra problem, we can define &lt;script type=&quot;math/tex&quot;&gt;\vec {A}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \vec{x_i^T}  \in R^{(d+1)*(d+1)}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec {b}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \cdot y_i \in R^{d+1}&lt;/script&gt;

&lt;p&gt;So &lt;script type=&quot;math/tex&quot;&gt;\vec {A} \cdot \vec {w} = \vec {b}&lt;/script&gt;. If A is invertible, then &lt;script type=&quot;math/tex&quot;&gt;\vec {w} =\vec {A^{-1}} \vec {b}&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;solve-lr-using-gradient-descent&quot;&gt;Solve LR using gradient descent&lt;/h5&gt;

&lt;p&gt;In     gradient descent, we would initialize the weights $\vec {w}$ at some random initialization and iteratively update $\vec {w}$ until we reach a point where the gradient is near 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\in-post\Rl\rl_gd.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Analysis of time cost:&lt;/p&gt;

&lt;p&gt;Using gradient descent can be more quick than the linear algebra solution, each gradient descent update costs O(nd) and the solution to the linear system costs O(d^3 + d^2n). Constructing A costs O(d^2n) and inversing matrix using O(d^3). For gradient descent, the k iteration costs O(ndk).&lt;/p&gt;

&lt;p&gt;To improve the efficient, we use stochastic gradient descent.&lt;/p&gt;

&lt;h4 id=&quot;stochastic-gradient-descent-and-handling-big-data-sets&quot;&gt;Stochastic gradient descent and handling Big Data sets&lt;/h4&gt;

&lt;p&gt;In stochastic approximation, we only use one sample or a small mini-batch of b samples (e.g., b = 32). For example, if we only use one sample, we can use this point to update the gradient:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
w_{t+1} &lt;-- w_{t} - n_t(\vec{x_i^T}-y_i)\vec{x_i} %]]&gt;&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Mar 2020 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/2020/03/18/Regression/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/18/Regression/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Formalizing Parameter Estimation</title>
        <description>&lt;p&gt;We want to find some function or model which satisfied some requirements. First, the model should be able to have performance on the previously unseen data should not deteriorate(change) once the new data is presented. Second, function must be able to include information about the model space from which it is selected and process of selecting a model should be able to accept training “advice” from an analyst. Finally, when large amounts of data are available learning algorithms must be able to provide solutions in reasonable time given the resource.&lt;/p&gt;

&lt;p&gt;In this section, we can use MAP and MLE to estimate some distribution’s parameters (mean μ and SD σ for normal distribution λ for Poisson distribution.)&lt;/p&gt;

&lt;h4 id=&quot;map-and-maximum-likelihood-estimationmle&quot;&gt;MAP and Maximum likelihood Estimation(MLE)&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Imagine there is a dataset of observations D={x&lt;sub&gt;i&lt;/sub&gt;} (i from 1 to n). It has an unknown but true distribution p*. However, if you know the distribution is in a set of possible distribution, F, F is called the hypothesis space or function class. For example, F can be the family of all univariate Gaussian distributions. In this family, the mean μ* and SD σ* can be different. F = {N(μ, σ^2)&lt;/td&gt;
      &lt;td&gt;for any μ ∈ R and σ ∈ R+}. While the true distribution has parameters μ*, σ*, we need to find μ and σ as close to the true ones as possible.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The idea of MAP estimation is to find the most probable model for the observed data. Given the dataset D, the solution of MAP is f&lt;sub&gt;MAP&lt;/sub&gt; = argmax&lt;sub&gt; f∈F&lt;/sub&gt; p(f&lt;/td&gt;
      &lt;td&gt;D).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(f&lt;/td&gt;
      &lt;td&gt;D) is called the posterior distribution of the model given the data. MAP estimate is exactly the most probable model.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To calculate the posterior distribution we apply he Bayes rules:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(f&lt;/td&gt;
      &lt;td&gt;D) = p(D&lt;/td&gt;
      &lt;td&gt;f)p(f)/p(D)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(D&lt;/td&gt;
      &lt;td&gt;f) is called the likelihood function, p(f) is the prior distribution of the model. p(D) is the marginal distribution of the data.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To compute p(D), we need to sum all situations with different f ∈ F. p(D) = sum(p(D&lt;/td&gt;
      &lt;td&gt;f)*p(f)).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;max &lt;sub&gt;fF&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f)/p(D) = 1/p(D) * max &lt;sub&gt;f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f). P(D) is not related with the MAP solution. Now f&lt;sub&gt;MAP&lt;/sub&gt; = argmax&lt;sub&gt; f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f)p(f). If in some situations, we cannot prefer one model and have the same p(f) for all f in the hypothesis space. Then MAP becomes MLE. f&lt;sub&gt;MLE&lt;/sub&gt; = argmax &lt;sub&gt;f∈F&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;f).  (The p(f) is not considered). So MLE can be seen as a special case of MAP.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;EX: Suppose data set D = {2,5,9,5,4,8} is an i.i.d. sample from a Poisson distribution with a fixed but unknown parameter λ&lt;sub&gt;0&lt;/sub&gt;. Find the MLE solution of λ&lt;sub&gt;0&lt;/sub&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;λ&lt;sub&gt;MLE&lt;/sub&gt; = argmax p(D&lt;/td&gt;
      &lt;td&gt;λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;p(D&lt;/td&gt;
      &lt;td&gt;λ) = p({x&lt;sub&gt;i&lt;/sub&gt;}&lt;/td&gt;
      &lt;td&gt;λ) = p(x&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)p(x&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)*…*p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;To find a λ to make the p maximum. ln(p(D&lt;/td&gt;
      &lt;td&gt;λ)) = ln(p(x&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)p(x&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)*…*p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;= sum(ln(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In Poisson distribution p(x&lt;/td&gt;
      &lt;td&gt;λ)=λ^x*e^(-λ)/x!, ln(p(x&lt;/td&gt;
      &lt;td&gt;λ)) = ln(λ^x)+ln(e^(-λ))-ln(x!) = x*ln(λ)-λ-ln(x!)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;sum(ln(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ))) = sum(x&lt;sub&gt;i&lt;/sub&gt;ln(λ)) - nλ - sum(ln(x!))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;By computing the first derivative, we can compute the λ which makes ln.. max.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;d ln(p(D&lt;/td&gt;
      &lt;td&gt;λ))/d λ = (1/λ)sum(x&lt;sub&gt;i&lt;/sub&gt;) - n. In this case, n = 6. λ=sum(x&lt;sub&gt;i&lt;/sub&gt;)/n&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Ex of MAP: D = {2,5,9,5,4,8}. i.i.d. sample from Poisson with mean λ&lt;sub&gt;0&lt;/sub&gt;. We need to calculate the MAP estimate of λ&lt;sub&gt;0&lt;/sub&gt;. Some other information is given. Suppose the prior knowledge about λ&lt;sub&gt;0&lt;/sub&gt; can be expressed using a gamma distribution with k=3 and θ=1. Find the MAP of λ&lt;sub&gt;0&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;By gamma distribution, p(λ) = λ^(k-1)e^(-λ/θ)/θgamma&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;By Poisson distribution, p(x&lt;/td&gt;
      &lt;td&gt;λ) = λ^x*e^-λ/x!&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;λ&lt;sub&gt;MAP&lt;/sub&gt; = argmax &lt;sub&gt;λ∈(0, ∞)&lt;/sub&gt; p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ). Taking log to simplify.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ln p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ) = ln(p(D&lt;/td&gt;
      &lt;td&gt;λ)) + ln(p(λ)) = ln sum(p(x&lt;sub&gt;i&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;λ)) + ln p(λ)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ln p(λ) = ln λ^(k-1)e^(-λ/e)/θgamma(k) = ln  λ^(k-1) + ln e^(-λ/e) - ln θgamma(k) = (k-1) ln λ - (λ/θ) - ln θgamma(k)&lt;/p&gt;

&lt;p&gt;sum(ln (λ^x&lt;sub&gt;i&lt;/sub&gt;&lt;em&gt;e^-λ/x&lt;sub&gt;i&lt;/sub&gt;)) = sum(x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;ln λ - λ - ln x&lt;sub&gt;i&lt;/sub&gt;) = ln λ sum(x&lt;sub&gt;i&lt;/sub&gt;) - nλ - ln sum(x&lt;sub&gt;i&lt;/sub&gt;)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Taking derivate: d(ln p(D&lt;/td&gt;
      &lt;td&gt;λ)p(λ)) / d λ= sum(x&lt;sub&gt;i&lt;/sub&gt;)/λ - n + (k-1)/λ - 1/θ = 0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;sum(x&lt;sub&gt;i&lt;/sub&gt;) + (k-1) = (n + 1/θ)λ&lt;/p&gt;

&lt;p&gt;So λ&lt;sub&gt;MAP&lt;/sub&gt; = (k-1 + sum(x&lt;sub&gt;i&lt;/sub&gt;))/(n + 1/θ) = (3-1+(2+5+9+5+4+8))/(6+1) = 35/7 = 5&lt;/p&gt;

&lt;h5 id=&quot;the-effect-of-the-amount-of-data&quot;&gt;The effect of the amount of data&lt;/h5&gt;

&lt;p&gt;When the data amount is large, the result of MAP becomes closer to MLE. The importance of prior is reduced.&lt;/p&gt;

&lt;p&gt;An example of Poisson distribution: s&lt;sub&gt;n&lt;/sub&gt; = sum(x&lt;sub&gt;i&lt;/sub&gt;), which is from the random variable S&lt;sub&gt;n&lt;/sub&gt; = sum(X&lt;sub&gt;i&lt;/sub&gt;) . And suppose s&lt;sub&gt;n&lt;/sub&gt;/n^2 –&amp;gt; ∞ when n –&amp;gt; ∞.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\web1111\xichen1.github.io\img\MLE-MAP.JPG&quot; alt=&quot;MLE-MAP&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ex: Let D={x&lt;sub&gt;i&lt;/sub&gt;}, i from 1 to n. It is an i.i.d. sample from a normal distribution. To find the MLE of the parameters.&lt;/p&gt;

&lt;p&gt;By taking partial derivatives of μ, σ, we get the result μ&lt;sub&gt;MLE&lt;/sub&gt; = 1/n(sum(x&lt;sub&gt;i&lt;/sub&gt;)), σ&lt;sub&gt;MLE&lt;/sub&gt; = 1/n(sum(x&lt;sub&gt;i&lt;/sub&gt;-μ&lt;sub&gt;MLE&lt;/sub&gt;)^2).&lt;/p&gt;

&lt;h4 id=&quot;bayesian-estimation&quot;&gt;Bayesian estimation&lt;/h4&gt;

&lt;p&gt;Maximum a posterior and maximum likelihood approaches report the solution that is consistent with the mode of the posterior distribution and the likelihood function. But the MLE and MAP cannot deal with the skewed distributions and multimodal distributions. Bayesian can!&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Formalizing-Parmeter-Estimation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Formalizing-Parmeter-Estimation/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Proof of Alg.</title>
        <description>&lt;p&gt;If the code is written using recursion, prove correctness using induction.&lt;/p&gt;

&lt;h5 id=&quot;loop-invariant&quot;&gt;Loop invariant&lt;/h5&gt;

&lt;p&gt;For code written using loops, prove correctness by the loop invariant method.&lt;/p&gt;

&lt;p&gt;A loop-invariant is an assertion about the state of the code that is always true at the beginning of each loop-iteration. Two steps of using loop-invariant: 1. identify the LI. 2. prove the LI for initialization, maintenance, termination#1 (stop eventually), termination#2 (right result).&lt;/p&gt;

&lt;p&gt;One example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;FindSum(A,n)
	sum &amp;lt;-- A[1]
	j &amp;lt;-- 2
	while (j&amp;lt;=n)
		sum &amp;lt;-- sum + A[j]
		j &amp;lt;-- j+1
	return sum
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The LI is “At the beginning of each loop iteration, sum = (j-1)sum(i=1)A[i]”.&lt;/p&gt;

&lt;h6 id=&quot;the-proof-of-li&quot;&gt;The proof of LI&lt;/h6&gt;

&lt;p&gt;Initially: Before the loop begins sum = A[1] = A[1,…,(2-1)]&lt;/p&gt;

&lt;p&gt;Maintenance: Suppose that at the beginning of iteration j, sum=A[1]+A[2]+…+A[j-1]. Then at the beginning of iteration j+1. Sum&lt;sub&gt;after&lt;/sub&gt;=sum&lt;sub&gt;before&lt;/sub&gt;+A[j] = A[1]+A[2]+..+A[j] = A[1]+A[2]+..+A[j+1-1].&lt;/p&gt;

&lt;p&gt;Termination #1: The loop terminates as we only increase j, so eventually j&amp;gt;n.&lt;/p&gt;

&lt;p&gt;Termination #2: When the while-loop terminates, j=n+1, in that case LI implies sum = A[1]+…+A[n].&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Proof-of-Alg/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Proof-of-Alg/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note heap</title>
        <description>&lt;p&gt;Some alg and time complexity of heap:&lt;/p&gt;

&lt;h5 id=&quot;max-heapify---θlogn---θh&quot;&gt;Max-heapify =  Θ(logn) =  Θ(h)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Max-Heapify(A,i)
		**pre-conditition: tree rooted at A[i] is and almost-heap
	lc &amp;lt;-- leftchild(i)
	rc &amp;lt;-- rightchild(i)
	largest &amp;lt;-- i
	if (lc &amp;lt;= heapsize(A) and A[lc] &amp;gt; A[largest]) then
		largest &amp;lt;-- lc
	if (rc &amp;lt;= heapsize(A) and A[rc] &amp;gt; A[largest]) then
		largest &amp;lt;-- rc
	if (largest != i) then
		exchange A[i] &amp;lt;--&amp;gt; A[largest]
		Max-heapify(A,largest)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The WC is from the top of the heap to the bottom of the heap. Every time it costs Θ(1) and in total it is h*Θ(1) = Θ(h) = Θ(logn).&lt;/p&gt;

&lt;h5 id=&quot;build-max-heap---θn&quot;&gt;Build-Max-Heap =  Θ(n)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Build-Max-Heap(A)
		** turn an array into a heap
	heapsize(A) &amp;lt;-- length[A]
	for (i &amp;lt;-- floor(length[A]/2) downto 1) do
		Max-heapify(A,i)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;heap-sort---θnlogn&quot;&gt;Heap sort =  Θ(nlogn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-Sort(A,n)
		** sort an array using a heap
	Build-Max-Heap(A)
	heapsize &amp;lt;-- n
	while (heapsize &amp;gt; 1) do
		exchange A[1] &amp;lt;--&amp;gt; A[heapsize]
		heapsize &amp;lt;-- heapsize - 1
		Max-Heapify(A[1...heapsize],1)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For each position i = n, n-1, n-2, … , 2, Max-Heapify takes O(logn). Totally, it costs Θ(nlogn).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;C:\Users\ppx\Desktop\Capture.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;priority-queue&quot;&gt;Priority Queue&lt;/h4&gt;

&lt;h5 id=&quot;initializea--θn&quot;&gt;Initialize(A) = Θ(n)&lt;/h5&gt;

&lt;p&gt;Building a heap&lt;/p&gt;

&lt;h5 id=&quot;extract-maximuma--θlogn&quot;&gt;Extract-Maximum(A) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-Extract-Max(A)
		precondiction: A is not empty
	max &amp;lt;-- A[1]
	A[1] &amp;lt;-- A[heapsize[A]]
	heapsize[A] &amp;lt;-- heapsize[A] - 1
	if (heapsize[A]&amp;gt;0) then
		Max-Heapify(A,1)
	return max
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The exchange costs Θ(1) and max heapify costs Θ(logn).&lt;/p&gt;

&lt;h5 id=&quot;heap-increase-key-a-i-key--θlogn&quot;&gt;Heap-Increase-Key (A, i, key) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Heap-increase-Key(A, i, key)
		** precondiction: key &amp;gt;= A[i]
	A[i] &amp;lt;-- key
	while (i&amp;gt;1 and A[parent(i)]&amp;gt;A[i]) do
		exchange A[i] &amp;lt;--&amp;gt; A[parent(i)]
		i &amp;lt;-- Parent(i)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the bottom to the top of the heap, it takes Θ(h) = Θ(logn)&lt;/p&gt;

&lt;h5 id=&quot;insert-key-a-new_key--θlogn&quot;&gt;Insert-Key (A, new_key) = Θ(logn)&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Insert-Key(A,new_key)
	heapsize[A] &amp;lt;-- heapsize[A]+1
	A[heapsize[A]] &amp;lt;-- $$-/infty$$
	Heap-Increase-key(A, heapsize[A], key)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Note-of-heap/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Note-of-heap/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note QS</title>
        <description>&lt;h5 id=&quot;the-idea-of-quick-sort&quot;&gt;The idea of quick sort&lt;/h5&gt;

&lt;p&gt;​	– pick a pivot and compare it to all others&lt;/p&gt;

&lt;p&gt;​	– Rearrange A to be: [elements&amp;lt;= pivot, pivot, elements &amp;gt; pivot]&lt;/p&gt;

&lt;p&gt;​	– Recursively sort the subarrays of before and after elements&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;QuickSort(A,p,r)
		** sort the subarray A[p,...,r]
	if (p&amp;lt;r) then
		q &amp;lt;-- Partition(A,p,r)
			** Partition returns q such that
			** 1. A[q] is the pivot
			** 2. All elements &amp;lt;= pivot appear in A[p,...,q-1]
			** 3. All elements &amp;gt; pivot appear in A[q+1,...,r]
		QuickSort(A,p,q-1)
		QuickSort(A,q+1,r)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Partition(A,p,r)
	** last element, A[r], is the pivot key picked of the partition
	pivot &amp;lt;-- A[r]
	i &amp;lt;-- p-1
	for (j from p to r-1) do
		if (A[j] &amp;lt;= pivot) then
			i &amp;lt;-- i+1
			exchange A[i] &amp;lt;--&amp;gt; A[j]
	exchange A[i+1] &amp;lt;--&amp;gt; A[r]
	return i+1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;quick-sort-running-time&quot;&gt;Quick Sort running time&lt;/h5&gt;

&lt;p&gt;In each recurrence, there is n-1 Key comparisons because the pivot is compared with other n-1 keys.&lt;/p&gt;

&lt;p&gt;​			0,                                   n&amp;lt;=1&lt;/p&gt;

&lt;p&gt;T(n) =&lt;/p&gt;

&lt;p&gt;​			T(n&lt;sub&gt;1&lt;/sub&gt;)+T(n-1-n&lt;sub&gt;1&lt;/sub&gt;)+(n-1), n&amp;gt;=2&lt;/p&gt;

&lt;p&gt;n&lt;sub&gt;1&lt;/sub&gt; + 1 is the index of the pivot. So quick sort needs&lt;/p&gt;

&lt;p&gt;T(All element before pivot)  + T(All element after pivot) + (n-1)&lt;/p&gt;

&lt;p&gt;Notice that 0 &amp;lt;= n&lt;sub&gt;1&lt;/sub&gt; &amp;lt;= n-1. Because the pivot can be any element from A[1] to A[n].&lt;/p&gt;

&lt;h6 id=&quot;worst-case&quot;&gt;Worst case&lt;/h6&gt;

&lt;p&gt;When n1 is zero/n-1, the number of KC becomes T(0) + T(n-1) + (n-1). In the next level, the KC becomes n-2.&lt;/p&gt;

&lt;p&gt;T(n) = T(n-1) + (n-1)&lt;/p&gt;

&lt;p&gt;​		= T(n-1) + n-1 = T(n-2) + n-2 + n-1 = … = T(1) + 1 + 2 + 3 + … + n-1 = n(n-1)/2&lt;/p&gt;

&lt;p&gt;So T(n) =  Θ(n^2)&lt;/p&gt;

&lt;h6 id=&quot;almost-worst-case&quot;&gt;Almost worst case&lt;/h6&gt;

&lt;p&gt;When n&lt;sub&gt;1&lt;/sub&gt; is n-2 or 1, T(n) = T(1)+T(n-2)+(n-1)&lt;/p&gt;

&lt;p&gt;​											= T(n-2)+(n-1) = T(n-4)+(n-3)+(n-1) = T(1) + 1 + 3 +…+ (n-1)&lt;/p&gt;

&lt;p&gt;T(n) = (1+(n-1))n/4=n^2/4 = Θ(n^2)&lt;/p&gt;

&lt;h6 id=&quot;best-case&quot;&gt;Best case&lt;/h6&gt;

&lt;p&gt;If every time we can choose a pivot which makes # before and # after are same, then we can save all wasted KC.&lt;/p&gt;

&lt;p&gt;T(n) = 2*T((n-1)/2)+(n-1)&lt;/p&gt;

&lt;p&gt;Solving T(n) = 2T(n/2) + (n-1). Using master theorem second case, a = b =2, n^(log &lt;sub&gt;b&lt;/sub&gt;a)=n. And f(n) = n-1 = n&lt;em&gt;((log^k)n) where k=0. So T(n) = (n&lt;/em&gt;logn).&lt;/p&gt;

&lt;h6 id=&quot;almost-best-case&quot;&gt;Almost best case&lt;/h6&gt;

&lt;p&gt;Assume in each round, # before is 3n/4 and # after is n/4. Then T(n) = T(3n/4)+T(n/4)+(n-1). Or more extreme case is T(n) = T(9n/10)+T(n/10)+(n-1).&lt;/p&gt;

&lt;p&gt;In both case, the time is O(n*logn). For any split, the running time remains to be  Θ(nlogn).&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Note-of-QS/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Note-of-QS/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Divide and Conquer</title>
        <description>&lt;h5 id=&quot;merge-sort&quot;&gt;Merge-Sort&lt;/h5&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Merge(A, lo, mid, hi)
	**pre-condition: lo&amp;lt;=mid&amp;lt;=hi, A[lo,mid] and A[mid+1,hi] sorted
	**post-condition: A[lo,hi] sorted
	
Merge-sort(A,lo,hi)
	if lo&amp;lt;hi then
		mid = floor((lo+hi)/2)
		Merge-Sort(A,lo,mid)
		Merge-sort(A,mid+1,hi)
		Merge(A,lo,mid,hi)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let denote #KC for a list of size n. Suppose the number of keys in the list is a power of 2.&lt;/p&gt;

&lt;p&gt;T(n)=(n-1)+2*T(n/2).&lt;/p&gt;

&lt;p&gt;To solve it: 1. Iterated substitution. 2. recurrence Tree 3. Guess and test. 4 Master Theorem&lt;/p&gt;

&lt;p&gt;T(n) = 2T(n/2) + (n-1) = 2&lt;em&gt;(2&lt;/em&gt;T(n/4)+(n/2-1))+(n-1)=4T(n/4)+(n-2)+(n-1) = 4(2T(n/8)+(n/4-1))+(n-2)+(n-1) = 8T(n/8)+(n-4)+(n-2)+(n-1) = 2^k*T(n/2^k)+(n-2^(k-1))+(n-2^(k-2))+…+(n-2^0)&lt;/p&gt;

&lt;p&gt;The last one is T(1)=T(n/2^k) So let 2^k=n. T(n)=2^k&lt;em&gt;T(1) + (2^k-2^(k-1)) + (2^k-2^(k-2)) + … + (2^k-2^0)= k&lt;/em&gt;2^k-sum(2^i) (0&amp;lt;=i&amp;lt;=k-1) = k*2^k-(2^k-1)=(k-1)2^k+1.&lt;/p&gt;

&lt;p&gt;2^k=n, k=logn. T(n) = (logn-1)n+1 =  O(nlogn)&lt;/p&gt;

&lt;p&gt;To prove by induction: T(n) = (k-1)2^k+1    n = 2^k&lt;/p&gt;

&lt;p&gt;Base case: T(1) = 0. k = 0, (0-1)2^0+1 = 1-1=0&lt;/p&gt;

&lt;p&gt;Induction step. Suppose T(2^k) = (k-1)2^k+1, we need to prove that T(2^(k+1))=(k)2^(k+1)+1&lt;/p&gt;

&lt;p&gt;T(2^(k+1))=(2^(k+1)-1)+2&lt;em&gt;T(2^k)= (2^(k+1)-1)+2&lt;/em&gt;(k-1)2^k+2= 2^k+(2k-2)2^k+1=(k)*2^(k+1)+1&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/23/Divide-and-Conquer/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/23/Divide-and-Conquer/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note of ML Optimization</title>
        <description>&lt;h4 id=&quot;the-basic-optimization-problem-and-stationary-points&quot;&gt;The basic optimization problem and stationary points&lt;/h4&gt;

&lt;p&gt;The goal of optimization is to find and select a set of parameters $ w \in R&lt;sup&gt;d &lt;/sup&gt;$ to minimize the objective function c:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min \ c(w)\ (w\in R^d)&lt;/script&gt;

&lt;p&gt;The way to select and find w which minimizes the objective. The most straightforward way is to generate random w and check c(w). If any new generated w&lt;sub&gt;t&lt;/sub&gt; has better performance than the previous best w, c(w&lt;sub&gt;t&lt;/sub&gt;) &amp;lt; c(w), then w&lt;sub&gt;t&lt;/sub&gt;  to be the new optimal solution.&lt;/p&gt;

&lt;p&gt;For the smooth and continuous function, we can use gradient descent (梯度下降). The gradient (derivative) can tell us where the derivative is zero and this place is somewhere the c(w)’s trend changes.&lt;/p&gt;

&lt;p&gt;By Taylor series approximation 
&lt;script type=&quot;math/tex&quot;&gt;c(w)=\sum_{n=0}^{\infty}{((c^n)(w_n)/n!)(w-w_0)^n}&lt;/script&gt;
The first three terms’ sum is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c(w)\approx \hat{c}(w)=c(w_0)+(w-w_0)c'(w_0)+1/2(w-w_0)^2c''(w_0)&lt;/script&gt;

&lt;p&gt;To find the zero derivative of c(w), we need to let $ c’(w) = 0 $. So we only need the first two terms.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;c'(w)\approx c'(w_0)+(w-w_0)c''(w_0)=0&lt;/script&gt;
Then,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w=w_0-c'(w_0)/c''(w_0)&lt;/script&gt;
In the end, the formula for the gradient descent is w&lt;sub&gt;t+1&lt;/sub&gt; =w&lt;sub&gt;t&lt;/sub&gt; - n&lt;sub&gt;t&lt;/sub&gt; c’(w&lt;sub&gt;t&lt;/sub&gt;).&lt;/p&gt;

&lt;p&gt;n&lt;sub&gt;t&lt;/sub&gt; is the rate of learning. In this formula, the new x-value = the old x-value  -  learning rate*old derivate.&lt;/p&gt;

&lt;p&gt;The way to choose n&lt;sub&gt;t&lt;/sub&gt;. We can conduct a line search. Set a range for the n, (0, n&lt;sub&gt;max&lt;/sub&gt;], from the n&lt;sub&gt;max&lt;/sub&gt;, we can reduce n if the step size is too large by changing the factor t which belongs to [0.5,0.9]. When t is 0.9, it reduces n very slowly and when t is 0.5, it reduces n very quickly.&lt;/p&gt;

&lt;h4 id=&quot;optimization-properties&quot;&gt;Optimization properties&lt;/h4&gt;

&lt;h5 id=&quot;maximizing-vs-minimizing&quot;&gt;Maximizing vs. minimizing&lt;/h5&gt;

&lt;p&gt;argmin c(w) = argmax (-c(w))&lt;/p&gt;

&lt;p&gt;It means that the value w which makes c(w) minimal is same as the value w which makes -c(w) maximum. However, note that min c(w) != max -c(w).&lt;/p&gt;

&lt;h5 id=&quot;convexity&quot;&gt;Convexity&lt;/h5&gt;

&lt;p&gt;A function is convex if we draw a line between any two points on the function and the value between the two points lie below the line. Convexity makes sure that every critical point is local minimum. And therefore, the solution we get from the right step size is always optimal. The concave function is all values between two points are above the line.&lt;/p&gt;

&lt;h5 id=&quot;uniqueness-of-the-solution&quot;&gt;Uniqueness of the solution&lt;/h5&gt;

&lt;p&gt;For some questions, we need to find the unique solution but sometimes we do not need to.&lt;/p&gt;

&lt;h5 id=&quot;equivalence-under-a-constant-shift&quot;&gt;Equivalence under a constant shift&lt;/h5&gt;

&lt;p&gt;Adding or multiplying by a constant a != 0 does not change the solution:
&lt;script type=&quot;math/tex&quot;&gt;argmin \ c(w)=argmin (\ a\ c(w)) = argmin (\ a\ c(w)+a )&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/22/Note-ML-Optimization/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/22/Note-ML-Optimization/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note of ML Estimator</title>
        <description>&lt;h4 id=&quot;estimate-the-expected-value&quot;&gt;Estimate the expected value&lt;/h4&gt;

&lt;p&gt;For example, we have n random variables X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;, … X&lt;sub&gt;n&lt;/sub&gt;, where E[X&lt;sub&gt;i&lt;/sub&gt;]= \mu and the mean \mu is unknown. The average estimator is 
&lt;script type=&quot;math/tex&quot;&gt;\bar{X}=(1/n)\sum_{i=1}^{n}{X_i}&lt;/script&gt;
How can we check if the estimator is biased? The bias of the estimator is how far the expected value of the estimator from the true \mu. In formula,
&lt;script type=&quot;math/tex&quot;&gt;Bias(\bar{X})=E[\bar{X}]-\mu&lt;/script&gt;
If the bias value if zero, the estimator is said to be unbiased. The E(\bar{x}) can reflect that the \bar{X} is random. Because if several experiments are conducted, there could be different E[\bar{X}].&lt;/p&gt;

&lt;p&gt;We can compute the expectation of \bar{X}.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{split}
E[\bar{X}]&amp;= E[(1/n)\sum_{i=1}^{n}{X_i}]\\
          &amp;= (1/n)\sum_{i=1}^{n}{E[X_i]}\\
          &amp;= 1/n\sum_{i=1}^{n}{\mu}\\
          &amp;= (1/n)*n*\mu\\
          &amp;= \mu
\end{split} %]]&gt;&lt;/script&gt;
It means that the expected value estimator is unbiased because the Bias is 0.&lt;/p&gt;

</description>
        <pubDate>Sat, 22 Feb 2020 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2020/02/22/Note-ML-Estimator/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/22/Note-ML-Estimator/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
      <item>
        <title>Note of Berkeley DB</title>
        <description>&lt;h4 id=&quot;levels-of-abstraction-in-a-typical-dbmsdatabase-management-system&quot;&gt;Levels of abstraction in a typical DBMS(database management system)&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Data is stored in tables in conceptual schema.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;conceptual-schema&quot;&gt;Conceptual schema:&lt;/h5&gt;

&lt;p&gt;The conceptual schema describes the Database structure of the whole database for the community of users. This schema hides information about the physical storage structures and focuses on describing data types, entities, relationships, etc. And one database can only have one conceptual schema.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use high level SQL commands (DDL or DML)&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;ddl&quot;&gt;DDL:&lt;/h5&gt;

&lt;p&gt;Data Definition Language (DDL) is a standard for commands that define the different structures in a database. DDL statements create, modify, and remove database objects such as tables, indexes, and users. Common DDL statements are CREATE, ALTER, and DROP.&lt;/p&gt;

&lt;h5 id=&quot;dml&quot;&gt;DML:&lt;/h5&gt;

&lt;p&gt;A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. Common DML statements are SELECT, INSERT, UPDATE.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data is organized in files and indexes in physical schema&lt;/li&gt;
  &lt;li&gt;A DBMS uses suitable storage structure, index files and access paths to evaluate queries and return results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;berkeley-db&quot;&gt;Berkeley DB&lt;/h5&gt;

&lt;p&gt;It is an open source embedded database library that provides s simple function-call API for data access and management.&lt;/p&gt;

&lt;p&gt;It supports many file structures and operations&lt;/p&gt;

&lt;h6 id=&quot;the-use-of-berkeley-db&quot;&gt;The use of Berkeley DB:&lt;/h6&gt;

&lt;ol&gt;
  &lt;li&gt;Develop programs working with storage structures and indexes&lt;/li&gt;
  &lt;li&gt;Develop applications that don’t need the full functionality of a DBMS and require high performance&lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;four-file-organizations-supported-in-berkeley-db&quot;&gt;Four file organizations supported in Berkeley DB&lt;/h6&gt;

&lt;ol&gt;
  &lt;li&gt;hash: Data is stored in an extended linear hash table. good for applications that need quick random-access&lt;/li&gt;
  &lt;li&gt;Btree: Data is stored in a sorted, balanced tree structure. Good for range-based searches&lt;/li&gt;
  &lt;li&gt;Queue: Data is stored in a queue as fixed-length records. Good for fast inserts at the tail of the queue. It also supports read/delete operation from the head of the queue. It provides record-level locking&lt;/li&gt;
  &lt;li&gt;Recnos: Data is stored in either fixed or variable-length records&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 14 Nov 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2019/11/14/Note-Berkeley-DB/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/14/Note-Berkeley-DB/</guid>
        
        <category>Study note</category>
        
        
      </item>
    
  </channel>
</rss>
