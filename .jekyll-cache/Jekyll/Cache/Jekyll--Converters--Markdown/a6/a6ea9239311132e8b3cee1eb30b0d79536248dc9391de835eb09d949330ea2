I"U<p>To learn the relationship between features and targets (x_i and y_i), we usually hypothesize the functional form of the relationship. The function can be linear or non-linear. e.g. f(X) = w0+w1x1+w2x2 and f(X) = a + bX1X2. The w0 w1 w2 and a and b are the parameters need to be learned.</p>

<p>In particular, the linear function is modeled as a linear combination of features (x) and parameters (w), i.e. f(X) = w0 + w1x1 + … + wdxd = sum(wjxj) = x^T w. We can extend x as (x0=1, x1, x2 …  xd). This form allows us use the dot product. Finding the best parameters (w0 to wd) isreferred to as the linear regression problem.</p>

<h4 id="formalizing-the-maximum-likelihood-problem">Formalizing the maximum likelihood problem</h4>

<p>Assume the observed data set D is a product of a data generating process in which n data points were drawn independently and according to the same distribution p(x). Assume the target variable Y has the linear relationship with input variable X, existing some error term  Ɛ. And the Ɛ follows the Gaussian distribution where the mean is zero. <script type="math/tex">Y =  \sum_{j=0}^{d} w_j X_j + Ɛ</script> . X0=1 is the intercept term. The assumption of normality for the error term is reasonable because of the central limit theorem.
<script type="math/tex">p(y|x,w) = (1/\sqrt(2\pi\sigma^2))exp(-(y-x^Tw)^2/2\sigma^2)</script></p>

<p>To implement the MLE problem,
<script type="math/tex">W_{MLE}=argmin_{w\in F} - \sum_{i=1}^n ln(p(y_i|x_i,w))
= argmin_{w\in F} \sum_{i=1}^n ln\sqrt{2\pi\sigma^2}+\sum_{i=1}^n (y_i-x_i^Tw)^2/2\sigma^2</script>
Only the second part is related with w, so
<script type="math/tex">= argmin_{w\in F} \frac{1} {2\sigma^2}*\sum_{i=1}^n (y_i-x_i^Tw)^2 \ = argmin_{w\in F}\sum_{i=1}^n (y_i-x_i^Tw)^2</script>
To find the w, we need to minimize the difference between real y_i and <script type="math/tex">\hat{y_i}</script> which is <script type="math/tex">x_i^T w</script>. So this follows the intuition.</p>

<p>E.g. Consider data set {(1,1.2), (2, 2.3), (3, 2.3), (4, 3.3)}. We want to find the max likelihood coefficients for f(x) = w_0 + w_1x. Note that there is one column in X which is ones because that can make w_0 times 1.</p>

<script type="math/tex; mode=display">% <![CDATA[
X= \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1&3 \\ 1&4\\ \end{bmatrix}     w = \begin{bmatrix} w_0  \\ w_1 \\ \end{bmatrix}   y =  \begin{bmatrix} 1.2  \\ 2.3 \\ 2.3 \\ 3.3 \\ \end{bmatrix} %]]></script>

<p>In the next section, we can use some new equation to get the answer of w.</p>

<h4 id="linear-regression-solution">Linear Regression solution</h4>

<p>We define the squared errors <script type="math/tex">c_i(W) = (f(x_i) - y_i)^2 = \frac{1} {2} (x_i^Tw - y_i)^2</script>. And <script type="math/tex">c(w) = \frac {1} {n} \sum{i=1}^n c_i(w)</script>. In this case, we can get the average squared error instead of a cumulative error. And this is same in the MLE problem because 1/n does not affect the w’s effect in the equation.</p>

<p>To solve the MLE problem, we usually get the gradient of the equation, <script type="math/tex">\nabla c(w) = \frac {1} {n} \sum_{i=1}^n \nabla c_i(w)</script></p>

<p><img src="C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\LR-solution.JPG" alt="" /></p>

<p>This is the result for c_i and w_j. So for the c(w), we need to obtain the system of equations with d+1 variables and d+1 equations.
<script type="math/tex">\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i0}=0\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{i1}=0\\...\\
\frac{1}{n} \sum_{i=1}^n(\vec{x_i^T}w - y_i)x_{id}=0</script>
Which is same as 
<script type="math/tex">\frac{1}{n} \sum_{i=1}^n (\vec{x_i^T}w - y_i)\vec{x_i} = 0</script></p>

<p>When we turn it as a linear algebra problem, we can define <script type="math/tex">\vec {A}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \vec{x_i^T}  \in R^{(d+1)*(d+1)}</script></p>

<script type="math/tex; mode=display">\vec {b}=\frac{1}{n} \sum_{i=1}^n \vec{x_i} \cdot y_i \in R^{d+1}</script>

<p>So <script type="math/tex">\vec {A} \cdot \vec {w} = \vec {b}</script>. If A is invertible, then <script type="math/tex">\vec {w} =\vec {A^{-1}} \vec {b}</script>.</p>

<h5 id="solve-lr-using-gradient-descent">Solve LR using gradient descent</h5>

<p>In     gradient descent, we would initialize the weights $\vec {w}$ at some random initialization and iteratively update $\vec {w}$ until we reach a point where the gradient is near 0.</p>

<p><img src="C:\Users\ppx\Desktop\ppx\xichen1.github.io\img\in-post\Rl\rl_gd.JPG" alt="" /></p>

<p>Analysis of time cost:</p>

<p>Using gradient descent can be more quick than the linear algebra solution, each gradient descent update costs O(nd)</p>
:ET