I"Ù<p>We want to find some function or model which satisfied some requirements. First, the model should be able to have performance on the previously unseen data should not deteriorate(change) once the new data is presented. Second, function must be able to include information about the model space from which it is selected and process of selecting a model should be able to accept training ‚Äúadvice‚Äù from an analyst. Finally, when large amounts of data are available learning algorithms must be able to provide solutions in reasonable time given the resource.</p>

<p>In this section, we can use MAP and MLE to estimate some distribution‚Äôs parameters (mean Œº and SD œÉ for normal distribution Œª for Poisson distribution.)</p>

<h4 id="map-and-maximum-likelihood-estimationmle">MAP and Maximum likelihood Estimation(MLE)</h4>

<table>
  <tbody>
    <tr>
      <td>Imagine there is a dataset of observations D={x<sub>i</sub>} (i from 1 to n). It has an unknown but true distribution p*. However, if you know the distribution is in a set of possible distribution, F, F is called the hypothesis space or function class. For example, F can be the family of all univariate Gaussian distributions. In this family, the mean Œº* and SD œÉ* can be different. F = {N(Œº, œÉ^2)</td>
      <td>for any Œº ‚àà R and œÉ ‚àà R+}. While the true distribution has parameters Œº*, œÉ*, we need to find Œº and œÉ as close to the true ones as possible.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>The idea of MAP estimation is to find the most probable model for the observed data. Given the dataset D, the solution of MAP is f<sub>MAP</sub> = argmax<sub> f‚ààF</sub> p(f</td>
      <td>D).</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>p(f</td>
      <td>D) is called the posterior distribution of the model given the data. MAP estimate is exactly the most probable model.</td>
    </tr>
  </tbody>
</table>

<p>To calculate the posterior distribution we apply he Bayes rules:</p>

<table>
  <tbody>
    <tr>
      <td>p(f</td>
      <td>D) = p(D</td>
      <td>f)p(f)/p(D)</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>p(D</td>
      <td>f) is called the likelihood function, p(f) is the prior distribution of the model. p(D) is the marginal distribution of the data.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>To compute p(D), we need to sum all situations with different f ‚àà F. p(D) = sum(p(D</td>
      <td>f)*p(f)).</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>max <sub>fF</sub> p(D</td>
      <td>f)p(f)/p(D) = 1/p(D) * max <sub>f‚ààF</sub> p(D</td>
      <td>f)p(f). P(D) is not related with the MAP solution. Now f<sub>MAP</sub> = argmax<sub> f‚ààF</sub> p(D</td>
      <td>f)p(f). If in some situations, we cannot prefer one model and have the same p(f) for all f in the hypothesis space. Then MAP becomes MLE. f<sub>MLE</sub> = argmax <sub>f‚ààF</sub> p(D</td>
      <td>f).  (The p(f) is not considered). So MLE can be seen as a special case of MAP.</td>
    </tr>
  </tbody>
</table>

<p>EX: Suppose data set D = {2,5,9,5,4,8} is an i.i.d. sample from a Poisson distribution with a fixed but unknown parameter Œª<sub>0</sub>. Find the MLE solution of Œª<sub>0</sub>.</p>

<table>
  <tbody>
    <tr>
      <td>Œª<sub>MLE</sub> = argmax p(D</td>
      <td>Œª)</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>p(D</td>
      <td>Œª) = p({x<sub>i</sub>}</td>
      <td>Œª) = p(x<sub>1</sub></td>
      <td>Œª)p(x<sub>2</sub></td>
      <td>Œª)*‚Ä¶*p(x<sub>i</sub></td>
      <td>Œª)</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>To find a Œª to make the p maximum. ln(p(D</td>
      <td>Œª)) = ln(p(x<sub>1</sub></td>
      <td>Œª)p(x<sub>2</sub></td>
      <td>Œª)*‚Ä¶*p(x<sub>i</sub></td>
      <td>Œª))</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>= sum(ln(p(x<sub>i</sub></td>
      <td>Œª)))</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>In Poisson distribution p(x</td>
      <td>Œª)=Œª^x*e^(-Œª)/x!, ln(p(x</td>
      <td>Œª)) = ln(Œª^x)+ln(e^(-Œª))-ln(x!) = x*ln(Œª)-Œª-ln(x!)</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>sum(ln(p(x<sub>i</sub></td>
      <td>Œª))) = sum(x<sub>i</sub>ln(Œª)) - nŒª - sum(ln(x!))</td>
    </tr>
  </tbody>
</table>

<p>By computing the first derivative, we can compute the Œª which makes ln.. max.</p>

<table>
  <tbody>
    <tr>
      <td>d ln(p(D</td>
      <td>Œª))/d Œª = (1/Œª)sum(x<sub>i</sub>) - n. In this case, n = 6. Œª=sum(x<sub>i</sub>)/n</td>
    </tr>
  </tbody>
</table>

<p>Ex of MAP: D = {2,5,9,5,4,8}. i.i.d. sample from Poisson with mean Œª<sub>0</sub>. We need to calculate the MAP estimate of Œª<sub>0</sub>. Some other information is given. Suppose the prior knowledge about Œª<sub>0</sub> can be expressed using a gamma distribution with k=3 and Œ∏=1. Find the MAP of Œª<sub>0</sub>.</p>

<p>By gamma distribution, p(Œª) = Œª^(k-1)e^(-Œª/Œ∏)/Œ∏gamma</p>

<table>
  <tbody>
    <tr>
      <td>By Poisson distribution, p(x</td>
      <td>Œª) = Œª^x*e^-Œª/x!</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>Œª<sub>MAP</sub> = argmax <sub>Œª‚àà(0, ‚àû)</sub> p(D</td>
      <td>Œª)p(Œª). Taking log to simplify.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>ln p(D</td>
      <td>Œª)p(Œª) = ln(p(D</td>
      <td>Œª)) + ln(p(Œª)) = ln sum(p(x<sub>i</sub></td>
      <td>Œª)) + ln p(Œª)</td>
    </tr>
  </tbody>
</table>

<p>ln p(Œª) = ln Œª^(k-1)e^(-Œª/e)/Œ∏gamma(k) = ln  Œª^(k-1) + ln e^(-Œª/e) - ln Œ∏gamma(k) = (k-1) ln Œª - (Œª/Œ∏) - ln Œ∏gamma(k)</p>

<p>sum(ln (Œª^x<sub>i</sub><em>e^-Œª/x<sub>i</sub>)) = sum(x<sub>i</sub></em>ln Œª - Œª - ln x<sub>i</sub>) = ln Œª sum(x<sub>i</sub>) - nŒª - ln sum(x<sub>i</sub>)</p>

<table>
  <tbody>
    <tr>
      <td>Taking derivate: d(ln p(D</td>
      <td>Œª)p(Œª)) / d Œª= sum(x<sub>i</sub>)/Œª - n + (k-1)/Œª - 1/Œ∏ = 0</td>
    </tr>
  </tbody>
</table>

<p>sum(x<sub>i</sub>) + (k-1) = (n + 1/Œ∏)Œª</p>

<p>So Œª<sub>MAP</sub> = (k-1 + sum(x<sub>i</sub>))/(n + 1/Œ∏) = (3-1+(2+5+9+5+4+8))/(6+1) = 35/7 = 5</p>

<h5 id="the-effect-of-the-amount-of-data">The effect of the amount of data</h5>

<p>When the data amount is large, the result of MAP becomes closer to MLE. The importance of prior is reduced.</p>

<p>An example of Poisson distribution: s<sub>n</sub> = sum(x<sub>i</sub>), which is from the random variable S<sub>n</sub> = sum(X<sub>i</sub>) . And suppose s<sub>n</sub>/n^2 ‚Äì&gt; ‚àû when n ‚Äì&gt; ‚àû.</p>

<p><img src="C:\Users\ppx\Desktop\web1111\xichen1.github.io\img\MLE-MAP.JPG" alt="MLE-MAP" /></p>

<p>Ex: Let D={x<sub>i</sub>}, i from 1 to n. It is an i.i.d. sample from a normal distribution. To find the MLE of the parameters.</p>

<p>By taking partial derivatives of Œº, œÉ, we get the result Œº<sub>MLE</sub> = 1/n(sum(x<sub>i</sub>)), œÉ<sub>MLE</sub> = 1/n(sum(x<sub>i</sub>-Œº<sub>MLE</sub>)^2).</p>

<h4 id="bayesian-estimation">Bayesian estimation</h4>

<p>Maximum a posterior and maximum likelihood approaches report the solution that is consistent with the mode of the posterior distribution and the likelihood function. But the MLE and MAP cannot deal with the skewed distributions and multimodal distributions. Bayesian can!</p>

:ET