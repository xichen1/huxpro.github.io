---

layout:     post
title:    "Formalizing Parameter Estimation"
subtitle:   "CMPUT 296"
date:     2020-02-23
author:     "rick"
header-img: "img/post-c-language.png"
tags:
- Study note
---

We want to find some function or model which satisfied some requirements. First, the model should be able to have performance on the previously unseen data should not deteriorate(change) once the new data is presented. Second, function must be able to include information about the model space from which it is selected and process of selecting a model should be able to accept training "advice" from an analyst. Finally, when large amounts of data are available learning algorithms must be able to provide solutions in reasonable time given the resource.

In this section, we can use MAP and MLE to estimate some distribution's parameters (mean Î¼ and SD Ïƒ for normal distribution Î» for Poisson distribution.) 

#### MAP and Maximum likelihood Estimation(MLE)

Imagine there is a dataset of observations D={x<sub>i</sub>} (i from 1 to n). It has an unknown but true distribution p*. However, if you know the distribution is in a set of possible distribution, F, F is called the hypothesis space or function class. For example, F can be the family of all univariate Gaussian distributions. In this family, the mean Î¼\* and SD Ïƒ\* can be different. F = {N(Î¼, Ïƒ^2) | for any Î¼ âˆˆ R and Ïƒ âˆˆ R+}. While the true distribution has parameters Î¼\*, Ïƒ\*, we need to find Î¼ and Ïƒ as close to the true ones as possible.

The idea of MAP estimation is to find the most probable model for the observed data. Given the dataset D, the solution of MAP is f<sub>MAP</sub> = argmax<sub> fâˆˆF</sub> p(f|D).

p(f|D) is called the posterior distribution of the model given the data. MAP estimate is exactly the most probable model. 

To calculate the posterior distribution we apply he Bayes rules: 

p(f|D) = p(D|f)p(f)/p(D)

p(D|f) is called the likelihood function, p(f) is the prior distribution of the model. p(D) is the marginal distribution of the data.

To compute p(D), we need to sum all situations with different f âˆˆ F. p(D) = sum(p(D|f)*p(f)).

max <sub>fF</sub> p(D|f)p(f)/p(D) = 1/p(D) * max <sub>fâˆˆF</sub> p(D|f)p(f). P(D) is not related with the MAP solution. Now f<sub>MAP</sub> = argmax<sub> fâˆˆF</sub> p(D|f)p(f). If in some situations, we cannot prefer one model and have the same p(f) for all f in the hypothesis space. Then MAP becomes MLE. f<sub>MLE</sub> = argmax <sub>fâˆˆF</sub> p(D|f).  (The p(f) is not considered). So MLE can be seen as a special case of MAP.

EX: Suppose data set D = {2,5,9,5,4,8} is an i.i.d. sample from a Poisson distribution with a fixed but unknown parameter Î»<sub>0</sub>. Find the MLE solution of Î»<sub>0</sub>.

Î»<sub>MLE</sub> = argmax p(D|Î»)

p(D|Î») = p({x<sub>i</sub>}|Î») = p(x<sub>1</sub>|Î»)p(x<sub>2</sub>|Î»)\*...\*p(x<sub>i</sub>|Î»)

To find a Î» to make the p maximum. ln(p(D|Î»)) = ln(p(x<sub>1</sub>|Î»)p(x<sub>2</sub>|Î»)\*...\*p(x<sub>i</sub>|Î»))

= sum(ln(p(x<sub>i</sub>|Î»)))

In Poisson distribution p(x|Î»)=Î»^x*e^(-Î»)/x!, ln(p(x|Î»)) = ln(Î»^x)+ln(e^(-Î»))-ln(x!) = x*ln(Î»)-Î»-ln(x!)

sum(ln(p(x<sub>i</sub>|Î»))) = sum(x<sub>i</sub>ln(Î»)) - nÎ» - sum(ln(x!))

By computing the first derivative, we can compute the Î» which makes ln.. max.

d ln(p(D|Î»))/d Î» = (1/Î»)sum(x<sub>i</sub>) - n. In this case, n = 6. Î»=sum(x<sub>i</sub>)/n



Ex of MAP: D = {2,5,9,5,4,8}. i.i.d. sample from Poisson with mean Î»<sub>0</sub>. We need to calculate the MAP estimate of Î»<sub>0</sub>. Some other information is given. Suppose the prior knowledge about Î»<sub>0</sub> can be expressed using a gamma distribution with k=3 and Î¸=1. Find the MAP of Î»<sub>0</sub>. 

By gamma distribution, p(Î») = Î»^(k-1)e^(-Î»/Î¸)/Î¸gamma

By Poisson distribution, p(x|Î») = Î»^x*e^-Î»/x!

 Î»<sub>MAP</sub> = argmax <sub>Î»âˆˆ(0, âˆž)</sub> p(D|Î»)p(Î»). Taking log to simplify.

ln p(D|Î»)p(Î») = ln(p(D|Î»)) + ln(p(Î»)) = ln sum(p(x<sub>i</sub>|Î»)) + ln p(Î»)

ln p(Î») = ln Î»^(k-1)e^(-Î»/e)/Î¸gamma(k) = ln  Î»^(k-1) + ln e^(-Î»/e) - ln Î¸gamma(k) = (k-1) ln Î» - (Î»/Î¸) - ln Î¸gamma(k)

sum(ln (Î»^x<sub>i</sub>*e^-Î»/x<sub>i</sub>)) = sum(x<sub>i</sub>*ln Î» - Î» - ln x<sub>i</sub>) = ln Î» sum(x<sub>i</sub>) - nÎ» - ln sum(x<sub>i</sub>)

Taking derivate: d(ln p(D|Î»)p(Î»)) / d Î»= sum(x<sub>i</sub>)/Î» - n + (k-1)/Î» - 1/Î¸ = 0

sum(x<sub>i</sub>) + (k-1) = (n + 1/Î¸)Î»

So Î»<sub>MAP</sub> = (k-1 + sum(x<sub>i</sub>))/(n + 1/Î¸) = (3-1+(2+5+9+5+4+8))/(6+1) = 35/7 = 5



##### The effect of the amount of data

When the data amount is large, the result of MAP becomes closer to MLE. The importance of prior is reduced.

An example of Poisson distribution: s<sub>n</sub> = sum(x<sub>i</sub>), which is from the random variable S<sub>n</sub> = sum(X<sub>i</sub>) . And suppose s<sub>n</sub>/n^2 --> âˆž when n --> âˆž.

![MLE-MAP](C:\Users\ppx\Desktop\web1111\xichen1.github.io\img\MLE-MAP.JPG)

Ex: Let D={x<sub>i</sub>}, i from 1 to n. It is an i.i.d. sample from a normal distribution. To find the MLE of the parameters.

By taking partial derivatives of Î¼, Ïƒ, we get the result Î¼<sub>MLE</sub> = 1/n(sum(x<sub>i</sub>)), Ïƒ<sub>MLE</sub> = 1/n(sum(x<sub>i</sub>-Î¼<sub>MLE</sub>)^2).





#### Bayesian estimation

Maximum a posterior and maximum likelihood approaches report the solution that is consistent with the mode of the posterior distribution and the likelihood function. But the MLE and MAP cannot deal with the skewed distributions and multimodal distributions. Bayesian can!

